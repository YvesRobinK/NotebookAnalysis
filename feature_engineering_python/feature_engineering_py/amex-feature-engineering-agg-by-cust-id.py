#!/usr/bin/env python
# coding: utf-8

# ## AMEX Default Competition - Feature Engineering
# 
# 
# - This notebook does the following:
# 
#     - aggregating data by customer_ID
#         - get the min, max, mean, std, first, last for each numeric features
#             - then create a feature that checks if the last number if with 1.5 standard deviation of mean
#         - get the unique count for categorical features 
# 
#     
# - the raw data used for this notebook is generated by:
#     - Process Amex Train Data to Parquet Format: https://www.kaggle.com/code/xxxxyyyy80008/process-amex-train-data-to-parquet-format
#     - datasets can be accessed here:
#         - train file: https://www.kaggle.com/datasets/xxxxyyyy80008/amex-train-20220706
#         - test file: https://www.kaggle.com/datasets/xxxxyyyy80008/amex-test-20020706
#     
# - this notebook also used some insights from this notebook:
#      - AMEX - Train Data EDA - Dask for Fast Analysis: https://www.kaggle.com/code/xxxxyyyy80008/amex-train-data-eda-dask-for-fast-analysis
#      
# - the data output of this notebook can be accessed here:
#      - train data: https://www.kaggle.com/datasets/xxxxyyyy80008/amex-agg-data-rev2
#    
# 
# 

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# In[ ]:


import numpy as np
import pandas as pd
import gc
import copy
import os
import sys

from pathlib import Path
from datetime import datetime, date, time, timedelta
from dateutil import relativedelta

import pyarrow.parquet as pq
import pyarrow as pa

import dask.dataframe as dd


# In[ ]:


import warnings
warnings.filterwarnings("ignore")

pd.options.display.max_rows = 100
pd.options.display.max_columns = 100


import pytorch_lightning as pl
random_seed=1234
pl.seed_everything(random_seed)


# ## aggregate by customer id

# In[ ]:


all_cols = ['customer_ID', 'S_2', 'P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42', 'D_43', 'D_44', 'B_4', 'D_45', 'B_5', 'R_2', 'D_46', 'D_47', 'D_48', 'D_49', 'B_6', 'B_7', 'B_8', 'D_50', 'D_51', 'B_9', 'R_3', 'D_52', 'P_3', 'B_10', 'D_53', 'S_5', 'B_11', 'S_6', 'D_54', 'R_4', 'S_7', 'B_12', 'S_8', 'D_55', 'D_56', 'B_13', 'R_5', 'D_58', 'S_9', 'B_14', 'D_59', 'D_60', 'D_61', 'B_15', 'S_11', 'D_62', 'D_63', 'D_64', 'D_65', 'B_16', 'B_17', 'B_18', 'B_19', 'D_66', 'B_20', 'D_68', 'S_12', 'R_6', 'S_13', 'B_21', 'D_69', 'B_22', 'D_70', 'D_71', 'D_72', 'S_15', 'B_23', 'D_73', 'P_4', 'D_74', 'D_75', 'D_76', 'B_24', 'R_7', 'D_77', 'B_25', 'B_26', 'D_78', 'D_79', 'R_8', 'R_9', 'S_16', 'D_80', 'R_10', 'R_11', 'B_27', 'D_81', 'D_82', 'S_17', 'R_12', 'B_28', 'R_13', 'D_83', 'R_14', 'R_15', 'D_84', 'R_16', 'B_29', 'B_30', 'S_18', 'D_86', 'D_87', 'R_17', 'R_18', 'D_88', 'B_31', 'S_19', 'R_19', 'B_32', 'S_20', 'R_20', 'R_21', 'B_33', 'D_89', 'R_22', 'R_23', 'D_91', 'D_92', 'D_93', 'D_94', 'R_24', 'R_25', 'D_96', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'B_36', 'B_37', 'R_26', 'R_27', 'B_38', 'D_108', 'D_109', 'D_110', 'D_111', 'B_39', 'D_112', 'B_40', 'S_27', 'D_113', 'D_114', 'D_115', 'D_116', 'D_117', 'D_118', 'D_119', 'D_120', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126', 'D_127', 'D_128', 'D_129', 'B_41', 'B_42', 'D_130', 'D_131', 'D_132', 'D_133', 'R_28', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145']

id_feats = ['customer_ID']
date_col =  'S_2'
cat_feats = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68', 'B_31']
missing20 = ['D_87', 'D_88', 'D_108', 'D_111', 'D_110', 'B_39', 'D_73', 'B_42', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'R_9', 'B_29', 'D_106', 'D_132', 'D_49', 'R_26', 'D_76', 'D_66', 'D_42', 'D_142', 'D_53', 'D_82', 'D_50', 'B_17', 'D_105', 'D_56', 'S_9', 'D_77', 'D_43', 'S_27', 'D_46']
float_feats = ['R_17', 'B_40', 'R_27', 'S_18', 'B_13', 'B_33', 'R_20', 'S_6', 'R_23', 'R_16', 'B_24', 'D_125', 'D_44', 'D_91', 'D_71', 'P_2', 'B_15', 'D_103', 'S_12', 'D_144', 'D_123', 'D_94', 'D_70', 'D_39', 'P_4', 'S_23', 'R_12', 'S_5', 'D_72', 'B_27', 'B_6', 'D_89', 'D_143', 'D_80', 'B_3', 'B_28', 'R_11', 'B_14', 'B_1', 'D_124', 'D_109', 'B_25', 'B_36', 'B_5', 'B_18', 'D_61', 'R_13', 'B_37', 'S_7', 'D_104', 'B_26', 'B_4', 'R_6', 'D_133', 'B_21', 'S_19', 'D_115', 'R_18', 'D_45', 'D_69', 'S_24', 'D_84', 'S_17', 'B_12', 'D_52', 'R_24', 'D_127', 'R_14', 'D_113', 'D_83', 'D_141', 'B_10', 'S_22', 'D_96', 'R_15', 'S_25', 'D_54', 'D_60', 'D_59', 'S_11', 'R_8', 'D_74', 'R_4', 'D_118', 'D_62', 'B_7', 'S_15', 'B_2', 'R_28', 'S_26', 'D_119', 'D_86', 'D_81', 'D_93', 'R_3', 'B_16', 'B_9', 'D_107', 'D_78', 'D_140', 'S_13', 'B_11', 'D_47', 'R_1', 'D_55', 'R_22', 'D_102', 'D_112', 'D_131', 'B_32', 'R_10', 'R_7', 'R_19', 'D_41', 'D_130', 'B_23', 'R_5', 'D_121', 'B_19', 'P_3', 'B_8', 'D_79', 'D_122', 'S_3', 'R_25', 'D_92', 'D_58', 'D_51', 'B_41', 'S_8', 'B_22', 'D_139', 'R_2', 'D_48', 'D_145', 'D_129', 'B_20', 'S_16', 'S_20', 'D_128', 'D_75', 'D_65', 'R_21']
log_feats= ['B_13', 'S_5', 'B_27', 'B_3', 'B_5', 'B_18', 'B_4', 'D_115', 'D_45', 'D_60', 'D_118', 'R_28', 'S_26', 'D_119', 'B_11', 'D_102']


# In[ ]:


len(float_feats) + len(cat_feats) + len(missing20), len(all_cols)
set(all_cols)-set(float_feats + cat_feats + missing20), len(set(float_feats + cat_feats + missing20))


# In[ ]:


get_ipython().run_cell_magic('time', '', "train_file = '/kaggle/input/amex-train-20220706/train.parquet'\n\ntest_file = '/kaggle/input/amex-test-20020706/amex_test_20220706.parquet'\n")


# ## Train file processing

# In[ ]:


get_ipython().run_cell_magic('time', '', 'agg_files = []\nstats = []\nfor c in set(float_feats + missing20):\n    df = dd.read_parquet(train_file, columns=[\'customer_ID\', \'S_2\',c], engine=\'pyarrow\')\n    x = df.compute().sort_values(by=\'S_2\', ascending=True).groupby("customer_ID").agg({c: [\'min\', \'max\', \'mean\', \'std\', \'first\',\'last\']})\n    x.columns = [f\'{c1}|{c2}\' for c1, c2 in x.columns]\n    \n    x[f\'{c}_mean2std\'] = (x[f\'{c}|last\'] >= (x[f\'{c}|mean\']-1.5*x[f\'{c}|std\'])) & (x[f\'{c}|last\'] <= (x[f\'{c}|mean\']+1.5*x[f\'{c}|std\']))\n    x[f\'{c}_mean2std\'] = x[f\'{c}_mean2std\'].astype(int)\n\n    pq.write_table(pa.Table.from_pandas(x[[f\'{c}|min\', f\'{c}|max\', f\'{c}|mean\', f\'{c}|last\',f\'{c}_mean2std\']]), \n                   f\'{c}.parquet\', compression = \'GZIP\')\n    agg_files.append(f\'{c}.parquet\')\n    \n    #--calculate the stats ------------------------------------------------------------------\n\n    cc_ = f\'{c}|last\'\n    item = [c, cc_, x[cc_].min(), x[cc_].max(), x[cc_].mean(), x[cc_].std(), x[cc_].median(), x[cc_].skew(), x[cc_].kurtosis()]\n\n    cc_ = f\'{c}|mean\'\n    item.extend([cc_, x[cc_].min(), x[cc_].max(), x[cc_].mean(), x[cc_].std(), x[cc_].median(), x[cc_].skew(), x[cc_].kurtosis()])\n\n    stats.append(item)\n')


# In[ ]:


stats_cols = ['feat']
stats_cols.extend([f'last_{c}' for c in ['feat', 'min', 'max', 'mean', 'std', 'median', 'skew', 'kurtosis']])
stats_cols.extend([f'mean_{c}' for c in ['feat', 'min', 'max', 'mean', 'std', 'median', 'skew', 'kurtosis']])

stats_df = pd.DataFrame(stats, columns= stats_cols )


stats_df.to_csv('train_agg_stats_rev2.csv', sep='|', index=False)


# In[ ]:


cat_feats.index(c), len(cat_feats)


# In[ ]:


get_ipython().run_cell_magic('time', '', 'for c in cat_feats:\n    df = dd.read_parquet(train_file, columns=[\'customer_ID\', c], engine=\'pyarrow\')\n    x0 = df.compute().groupby("customer_ID")[c].value_counts().to_frame()\n    x0=x0.unstack().fillna(0)\n    x0.columns = [f\'{c0}={c1}\' for c0, c1 in x0.columns]\n    \n    df = dd.read_parquet(train_file, columns=[\'customer_ID\', \'S_2\',c], engine=\'pyarrow\')\n    x1 = df.compute().sort_values(by=\'S_2\', ascending=True).groupby("customer_ID").agg({c: [\'nunique\',\'last\']})\n    x1.columns = [f\'{c1}|{c2}\' for c1, c2 in x1.columns]\n    \n    x = x0.merge(x1, left_index=True, right_index=True, how=\'left\')\n\n    pq.write_table(pa.Table.from_pandas(x), f\'{c}.parquet\', compression = \'GZIP\')\n    agg_files.append(f\'{c}.parquet\')\n')


# In[ ]:


def cal_days(v):
    m0 = v['S_2=min']
    m1 = v['S_2=max']
    if m1 is np.nan:
        m1 = m0
    
    return (datetime.strptime(m1, '%Y-%m-%d') - datetime.strptime(m0, '%Y-%m-%d')).days

        


# In[ ]:


get_ipython().run_cell_magic('time', '', 'for c in [\'S_2\']:\n    df = dd.read_parquet(train_file, columns=[\'customer_ID\', c], engine=\'pyarrow\')\n    x = df.compute().groupby("customer_ID").agg({c: [\'min\', \'max\', \'count\']})\n    x.columns = [f\'{c1}={c2}\' for c1, c2 in x.columns]   \n    \n    days = []\n    for _, row in x.iterrows():\n        days.append(cal_days(row))\n    x[\'days\']=days\n    \n    pq.write_table(pa.Table.from_pandas(x), f\'{c}.parquet\', compression = \'GZIP\')\n    agg_files.append(f\'{c}.parquet\')\n')

%%time
df = dd.read_parquet(train_file, columns=['customer_ID'], engine='pyarrow')
x = df.compute().groupby("customer_ID").size().to_frame()

pq.write_table(pa.Table.from_pandas(x), f'customer_ID.parquet', compression = 'GZIP')
agg_files.append(f'customer_ID.parquet')
# ## combine all files

# In[ ]:


len(agg_files), agg_files[:1], len(set(agg_files))


# In[ ]:


agg_files = list(set(agg_files))


# In[ ]:


df = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')
# df = df[['customer_ID']].copy(deep=True)

for i, file in enumerate(agg_files):
    df_ = pd.read_parquet(file).reset_index()
    df = df.merge(df_,on=['customer_ID'], how='left')
        
    del df_
    gc.collect()
        
print(i)
pq.write_table(pa.Table.from_pandas(df), f'agg_train_all_rev2.parquet', compression = 'GZIP')
del df
gc.collect()


# In[ ]:


len(agg_files)


# In[ ]:


for file in agg_files:
    Path(file).unlink()


# In[ ]:


files = next(os.walk('.'))[2]
files

