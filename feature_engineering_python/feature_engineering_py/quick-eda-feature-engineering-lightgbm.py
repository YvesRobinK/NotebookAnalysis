#!/usr/bin/env python
# coding: utf-8

# In[160]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split, RandomizedSearchCV
import lightgbm as lgb

import warnings
warnings.filterwarnings('ignore')


# # Load Datasets

# In[161]:


train = pd.read_csv('/kaggle/input/playground-series-s3e16/train.csv')
test = pd.read_csv('/kaggle/input/playground-series-s3e16/test.csv')


# In[162]:


print(f'Train size: {len(train)}')
print(f'Test size: {len(test)}')


# In[163]:


train.info()


# In[164]:


test.info()


# In[165]:


train.describe()


# In[166]:


test.describe()


# We do not need the `id` column in our trainset. But keep `id` column in the testset for submission. 

# In[167]:


train = train.drop(columns=['id'])
test_ids = test['id']
test = test.drop(columns=['id'])


# **Summary**
# - We have around 74000 observation in our training frame and around 50000 in our testset
# - Our target is the `Age` column
# - We have 8 potential features (1 categorical, all others are floats)
# - Luckily we have no missing values in our train and test set

# ----

# # Quick EDA
# 
# ## Target distribution

# In[168]:


fig = plt.figure(figsize=(10, 10))
sns.distplot(train['Age'])
plt.show();


# Our target looks normal distributed, no need for transforming it. 

# In[169]:


fig = plt.figure(figsize=(20, 5))
sns.boxplot(train['Age'])
plt.show();


# ## Categorical feature distribution

# In[170]:


plt.figure(figsize=(10, 8))
sns.countplot(data=train, x='Sex', palette='Set1')

# Count number of observations in each class
male, other, female = train['Sex'].value_counts()

print('No. male: ', male)
print('No. other : ', other)
print('No. female : ', female)
print('')
print('% of crabs labeled male', round(male / len(train) * 100, 2), '%')
print('% of crabs labeled other', round(other / len(train) * 100, 2), '%')
print('% of crabs labeled female', round(female / len(train) * 100, 2), '%')
plt.show();


# ## Numerical feature distribution
# 
# ### Pairplot

# In[171]:


nums = train[['Age', 'Length', 'Diameter', 'Height', 'Shucked Weight', 'Viscera Weight', 'Shell Weight']]


# In[172]:


sns.pairplot(data=nums, palette='Set1')
plt.show();


# There is almost a perfect linear pattern between `Length` and `Diameter` which are hinting at the presence of multicollinearity between these variables.
# In general, a strong correlation can be seen between almost all variables. This makes sense in any case, e.g. if a crab is heavier, then it is also bigger. 

# ### Histplot and Boxplot

# In[173]:


def plot_histogram_boxplot(df, target_variable):
    """
    Plots a histogram and a box plot for each variable in the dataframe against the target variable.
    
    Args:
        df (pandas.DataFrame): The dataframe containing the variables.
        target_variable (str): The name of the target variable in the dataframe.
    """
    for column in df.columns:
        if column != target_variable:
            plt.figure(figsize=(12, 4))
            
            # Histogram
            plt.subplot(1, 2, 1)
            plt.hist(df[column], bins=20)
            plt.xlabel(column)
            plt.ylabel('Count')
            plt.title('Histogram')
            
            # Box plot
            plt.subplot(1, 2, 2)
            plt.boxplot(df[column], vert=False)
            plt.xlabel(column)
            plt.ylabel('Value')
            plt.title('Box Plot')
            
            # Add target variable to the title
            plt.suptitle(f'{column} vs. {target_variable}')
            
            plt.tight_layout()
            plt.show()


# In[174]:


plot_histogram_boxplot(nums, 'Age')


# ### Correlation matrix

# In[175]:


corr = nums.corr().round(2)
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(10, 10))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.tight_layout()


# **Summary**
# - Our target is normal distributed, no need for transformation
# - `Sex` column is almost balanced
# - Several features are skewed - Since we are using LightGBM there is no need to transform them
# - Several features have outliers, we skip removing them because LightGBM is fairly robust against outliers
# - We have high correlation between variables itself and variables to the target which is natural

# ----

# # Quick Feature Engineering
# 
# There are many possibilities for feature engineering which can be tested. New features can be generated by combining several numeric variables, features that better describe the relationship between sex and a variable, and so on. I will only calculate the Body Mass Index and a view others. 

# In[176]:


# Calculate BMI
train['BMI'] = train['Weight'] / ((train['Height'] / 100) ** 2)
test['BMI'] = test['Weight'] / ((test['Height'] / 100) ** 2)


# In[177]:


train['Volume'] = train['Height'] * train['Weight'] * train['Diameter']
train['LogDens'] = np.log(train['Weight']) / train['Volume']


# In[178]:


test['Volume'] = test['Height'] * test['Weight'] * test['Diameter']
test['LogDens'] = np.log(test['Weight']) / test['Volume']


# In[179]:


print(f'Train size: {train.shape}')
print(f'Test size: {test.shape}')


# In[180]:


corr = train.corr().round(2)
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(15, 15))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.tight_layout()


# ---

# # Preprocessing
# 
# Only preprocessing step is to one-hot-encode our categorical feature `Sex`. 

# In[181]:


train = pd.get_dummies(train, columns=['Sex'])
test = pd.get_dummies(test, columns=['Sex'])


# ----

# # Baseline Model

# In[182]:


# Create features and target
X = train.drop(columns=['Age'])
y = train['Age']


# In[183]:


# split into train and test
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1702)


# In[184]:


params = {
    'task': 'train', 
    'boosting': 'gbdt',
    'objective': 'regression',
    'num_leaves': 15,
    'max_depth': 3,
    'learning_rate': 0.1,
    'metric': {'l2','l1'},
    'verbose': -1
}


# In[185]:


lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)


# In[186]:


model = lgb.train(params,
                 num_boost_round=2000,
                 train_set=lgb_train,
                 valid_sets=lgb_eval,
                 early_stopping_rounds=50
                 )


# In[187]:


y_pred = model.predict(X_val)


# In[188]:


print(f'MAE: {mean_absolute_error(y_val, y_pred)}')
print(f'MSE: {mean_squared_error(y_val, y_pred)}')
print(f'RMSE: {mean_squared_error(y_val, y_pred, squared=False)}')


# In[189]:


y_pred = np.round(y_pred, 2)


# In[190]:


print(f'MAE: {mean_absolute_error(y_val, y_pred)}')
print(f'MSE: {mean_squared_error(y_val, y_pred)}')
print(f'RMSE: {mean_squared_error(y_val, y_pred, squared=False)}')


# # Hyperparameter Tuning using RandomSearchCV

# In[191]:


param_grid = {
    'boosting_type': ['gbdt', 'dart'],
    'num_leaves': [10, 20, 30],
    'learning_rate': [0.01, 0.1, 0.5],
    'n_estimators': [100, 200, 300],
    'subsample': [0.5, 0.8, 1.0],
    'max_depth': [-1, 4, 8],
    'colsample_bytree': [0.5, 0.8, 1.0],
    'reg_alpha': [0.0, 0.1, 0.5],
    'reg_lambda': [0.0, 0.1, 0.5]
}


# In[192]:


regressor = lgb.LGBMRegressor()
random_search = RandomizedSearchCV(estimator=regressor, param_distributions=param_grid,
                                   n_iter=10, scoring='neg_mean_squared_error', cv=5)
random_search.fit(X_train, y_train)
print("Best Parameters: ", random_search.best_params_)
print("Best MSE: ", -random_search.best_score_)


# In[193]:


best_model = random_search.best_estimator_
y_pred = best_model.predict(X_val)


# In[194]:


y_pred = np.round(y_pred, 2)


# In[195]:


print(f'MAE: {mean_absolute_error(y_val, y_pred)}')
print(f'MSE: {mean_squared_error(y_val, y_pred)}')
print(f'RMSE: {mean_squared_error(y_val, y_pred, squared=False)}')


# ----

# # Submission

# In[196]:


sub_preds = model.predict(test)


# In[197]:


sub_df = pd.DataFrame({'id': test_ids, 'Age': sub_preds})


# In[198]:


sub_df


# In[199]:


sub_df['Age'] = sub_df['Age'].round().astype(int)


# In[200]:


sub_df


# In[201]:


sub_df.to_csv('submission.csv', index=False)


# In[ ]:




