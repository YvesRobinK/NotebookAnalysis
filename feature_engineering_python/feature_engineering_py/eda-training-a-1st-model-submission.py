#!/usr/bin/env python
# coding: utf-8

# In this competition we will be using data generated by a deep learning model trained on the [IBM HR Analytics Employee Attrition & Performance dataset](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset). We can expect the relationships between variables to be similar as in the original dataset, but not exactly the same.
# 
# We will be predicting employee attrition (whether an employee will quit or not) based on 33 variables: `Age`,
#  `BusinessTravel`,
#  `DailyRate`,
#  `Department`,
#  `DistanceFromHome`,
#  `Education`,
#  `EducationField`,
#  `EmployeeCount`,
#  `EnvironmentSatisfaction`,
#  `Gender`,
#  `HourlyRate`,
#  `JobInvolvement`,
#  `JobLevel`,
#  `JobRole`,
#  `JobSatisfaction`,
#  `MaritalStatus`,
#  `MonthlyIncome`,
#  `MonthlyRate`,
#  `NumCompaniesWorked`,
#  `Over18`,
#  `OverTime`,
#  `PercentSalaryHike`,
#  `PerformanceRating`,
#  `RelationshipSatisfaction`,
#  `StandardHours`,
#  `StockOptionLevel`,
#  `TotalWorkingYears`,
#  `TrainingTimesLastYear`,
#  `WorkLifeBalance`,
#  `YearsAtCompany`,
#  `YearsInCurrentRole`,
#  `YearsSinceLastPromotion`,
#  `YearsWithCurrManager`.
#  
#  The `id` column is non-overlapping between `train` and `test` and is meant for data organization purposes thus likely doesn't carry any signal.
# 
# The target (`Attrition`) is a binary variable. And as such, the evaluation metric for this competition has been selected to be the [area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.
# 
# As we only have 7 days to come up with a solution, I recorded this short walkthrough to help you get started üôÇ Please feel free to watch it if it might be helpful, otherwise, please skip over it. The notebook should stand on its own, I include plenty of descriptions, but the video might help provide additional context for newere members of the Kaggle community!

# In[1]:


from IPython.display import YouTubeVideo

YouTubeVideo('8CO7FnF2yNM', width=800, height=300)


# Let's begin by taking a look at the data.

# In[2]:


import pandas as pd
from pathlib import Path
import numpy as np


# In[3]:


DATAPATH = Path('../input/playground-series-s3e3')

train = pd.read_csv(DATAPATH/'train.csv')
test = pd.read_csv(DATAPATH/'test.csv')
sample_sub = pd.read_csv(DATAPATH/'sample_submission.csv')


# In[4]:


train.head()


# In[5]:


train.shape, test.shape


# An initial problem already stands out! There are relatively few training examples (1677) compared to the number of variables. That can easily lead to overfitting.
# 
# One of the main axis of competition in this challenge will be to find variables that generalize well to unseen data and to limit the overfitting.
# 
# Now for the the big question -- do we have any NAs?
# 
# And the answer is no!

# In[6]:


train.isna().sum()


# This is shaping up to be a really interesting challenge!
# 
# Let's train a baseline model and in the process take a first look at feature importances. This can guide our further exploration.

# # Training our first model (with ensembling)

# In[7]:


# eliminating annoying lgbm warnings, source: https://stackoverflow.com/a/33616192/1105837
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

from lightgbm.sklearn import LGBMClassifier
import lightgbm as lgbm
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold


# A very interesting finding from the previous PS Tabular Data competitions was that adding original data (the data on which the Deep Learning model that was used to generate the data for this competition was trained) helps.
# 
# Let's add the original data here and see if it improves the score! (we will be able to compare vs the earlier versions of this notebook).

# In[8]:


original_data = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')

# munging the original data to make it appear more like our train DataFrame
original_data = original_data.rename(columns={'EmployeeNumber': 'id'})
original_data['Attrition'] = (original_data['Attrition'] == 'Yes').astype(int)

# rearranging columns so that they are in the same order as in train
original_data = original_data[train.columns.tolist()]

# adding the source column
original_data['source'] = 'original'
train['source'] = 'synthetic'
test['source'] = 'synthetic'

# combining the datasets
train = pd.concat([train, original_data])
train.reset_index(inplace=True, drop=True)


# **RESULTS**: In the version where I added the original data, the scores improved from `0.88157` to `0.92841`. This is a dramatic improvement in performance! Adding in the original data most certainly is the way to go.
# 
# Let's define the features for our model to use for training.

# In[9]:


train.columns


# In[10]:


features = list(train.columns)
features.remove('id')
features.remove('Attrition')

target = 'Attrition'


# Since we have multiple columns with strings, let's encode them as integers. We can use this bit of code below to make our life easier:

# In[11]:


# source: https://stackoverflow.com/a/30267328/1105837

class MultiColumnLabelEncoder:
    def __init__(self,columns = None):
        self.columns = columns # array of column names to encode

    def fit(self,X,y=None):
        return self # not relevant here

    def transform(self,X):
        '''
        Transforms columns of X specified in self.columns using
        LabelEncoder(). If no columns specified, transforms all
        columns in X.
        '''
        output = X.copy()
        if self.columns is not None:
            for col in self.columns:
                output[col] = LabelEncoder().fit_transform(output[col])
        else:
            for colname,col in output.iteritems():
                output[colname] = LabelEncoder().fit_transform(col)
        return output

    def fit_transform(self,X,y=None):
        return self.fit(X,y).transform(X)


# Here are the string columns we will be encoding as integers.

# In[12]:


columns_with_strings_as_values = list((train.dtypes[train.dtypes == 'object']).index)
columns_with_strings_as_values


# In[13]:


label_encoder = MultiColumnLabelEncoder(columns=columns_with_strings_as_values)
train = label_encoder.fit_transform(train)
test = label_encoder.transform(test)


# In[14]:


clfs = []
scores = []


kf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)
for train_index, val_index in kf.split(train, y=train['Attrition']):
    X_train, X_val = train[features].loc[train_index], train[features].loc[val_index]
    y_train, y_val = train[target][train_index], train[target][val_index]
    
    clf = LGBMClassifier(n_estimators=150, categorical_feature=[1, 3, 6, 9, 13, 15, 19, 20, 33], metric='auc')
    clf.fit(X_train.values, y_train, eval_set=[(X_val, y_val)], verbose=False)
    preds = clf.predict_proba(X_val.values)
    
    clfs.append(clf)
    scores.append(roc_auc_score(y_val, preds[:, 1]))
print(f'mean score across all folds: {np.mean(scores)}')


# Let us now look at the variables that are important according to our model.

# In[15]:


for i in clf.feature_importances_.argsort()[::-1]:
    print(features[i], clf.feature_importances_[i]/clf.feature_importances_.sum())


# These are very interesting findings! Turns out how much you pay your employees actually matters! But it seems that the age of the employee and commute also play a significant role!
# 
# We only have 7 days to come up with a good solution! This overview can suggest which variables to look at first in the context of feature engineering.
# 
# Below, when creating the submission, we will ensemble our models. To test the water with adding more models, let's add predictions from Catboost into the mix!

# In[16]:


from catboost import CatBoostClassifier

scores = []
kf = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)
for train_index, val_index in kf.split(train, y=train['Attrition']):
    X_train, X_val = train[features].loc[train_index], train[features].loc[val_index]
    y_train, y_val = train[target][train_index], train[target][val_index]

    clf = CatBoostClassifier(iterations=200)
    clf.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)
    
    preds = clf.predict_proba(X_val.values)[:, 1]
    clfs.append(clf)
    scores.append(roc_auc_score(y_val, preds))
print(f'mean auc across all folds: {np.mean(scores)}')


# # Making a submission

# Lets being by predicting on the test set using each of our trained classifiers.

# In[17]:


test_preds = []

for clf in clfs:
    preds = clf.predict_proba(test[features].values)
    test_preds.append(preds[:, 1])


# We can now take the mean of our predictions.

# In[18]:


test_preds = np.stack(test_preds).mean(0)
test_preds


# In[19]:


submission = pd.DataFrame(data={'id': test.id, 'Attrition': test_preds})
submission.head()


# In[20]:


submission.to_csv('submission.csv', index=False)


# This is shaping up to be a very interesting challenge! ü•≥
# 
# **If you found this notebook useful, please upvote! üôè Thank you!**
# 
# All the best in the competition!
