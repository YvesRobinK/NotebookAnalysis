#!/usr/bin/env python
# coding: utf-8

# <div style="color: #fff7f7;
#            display:fill;
#            border-radius:10px;
#            border-style: solid;
#            border-color:#424949;
#            text-align:center;
#            background-color:#003a6c ;
#            font-size:15px;
#            letter-spacing:0.5px;
#            padding: 0.7em;
#            text-align:left" >
#     <h1 style="text-align:center;font-weight: 20px; color:White;">
#        Feature Processing and Model Selection of TitanicğŸ˜</h1>
# </div>

# In[1]:


import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
sns.set_theme()


# In[2]:


df_train = pd.read_csv('../input/titanic/train.csv')
df_test = pd.read_csv('../input/titanic/test.csv')
data = df_train.append(df_test)


# In[3]:


df_train.head()


# In[4]:


df_train.isnull().sum()


# In[5]:


df_test.isnull().sum()


# In[6]:


df_train.duplicated().sum()


# In[7]:


print("Training set size",len(df_train))
print("Test set size",len(df_train))


# In[8]:


data


# # 1 EDA
# <div style="color: #fff7f7;
#            display:fill;
#            border-radius:10px;
#            border-style: solid;
#            border-color:#424949;
#            text-align:center;
#            background-color:#003a6c ;
#            font-size:15px;
#            letter-spacing:0.5px;
#            padding: 0.7em;
#            text-align:left">
# <center style="font-size:30px">Some things that need to be explainedğŸ…</center>
# <hr>ç‰¹å¾å¤„ç†ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
# <li>1.å¹´é¾„å’Œç¥¨ä»·çš„åˆ†ç®±æ•°é‡è¦å°½é‡å¤šä¸€ç‚¹ã€‚
# <li>2.é€šè¿‡åå­—/ç¥¨ä»·ä¸¤ä¸ªç‰¹å¾è¿›è¡Œé…åˆï¼Œæ„å»ºä¸€ä¸ªè¡¨ç¤ºå®¶åº­æ­»äº¡çŠ¶æ€çš„ç‰¹å¾ã€‚
# <li>3.Cabinåˆ—åº”è¯¥ç•™ä¸‹ï¼Œç”¨å…¶ä»–å€¼æ ‡è¯†ç¼ºå¤±å€¼ï¼Œå®ƒæ˜¾è‘—çš„æå‡äº†0.2åˆ†ã€‚<hr>
# <li>1. The number of bins for age and fare should be as large as possible.
# <li>2. The two features of name/fare are combined to construct a feature that represents the death state of the family.
# <li>3. The Cabin column should be left, and other values are used to identify missing values, which significantly improves by 0.2 points.
# </div>

# In[9]:


g = sns.FacetGrid(data[:891],col="Pclass")
g.map_dataframe(sns.pointplot,x="Sex",y="Survived")


# - ç”Ÿå­˜æ¯”ä¾‹è·Ÿé˜¶å±‚å­˜åœ¨å…³ç³»ï¼Œåœ¨1/2é˜¶çº§ä¸­ï¼Œç”·æ€§çš„å­˜æ´»æ¯”ä¾‹è¦è¶…è¿‡å¥³æ€§ã€‚ä½†æ˜¯åœ¨3é˜¶çº§ä¸­ï¼Œå¥³æ€§çš„å­˜æ´»æ¯”ä¾‹è¦è¶…è¿‡ç”·æ€§ã€‚
# - The survival rate has a relationship with the class. In the 1/2 class, the survival rate of men exceeds that of women. But in the three classes, the survival rate of women exceeds that of men.

# In[10]:


g = sns.FacetGrid(data[:891],col="Pclass",hue="Survived")
g.map_dataframe(sns.kdeplot,x="Fare")
g.add_legend()


# - æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œ1é˜¶çº§çš„äººç¥¨ä»·åˆ†å¸ƒæ¯”è¾ƒå¹³ç¼“ï¼Œä¸”å­˜åœ¨å¾ˆé«˜çš„ç¥¨ä»·ï¼Œè¿™è¯´æ˜ç¥¨ä»·å­˜åœ¨ä¸¥é‡çš„å³åã€‚
# - We have noticed that the fare distribution of class 1 people is relatively flat, and there are high fares, which shows that the fares are seriously skewed to the right.

# In[11]:


g = sns.FacetGrid(data[:891],col="Sex",hue="Survived")
g.map_dataframe(sns.histplot,x="Age")
g.add_legend()


# - æ— è®ºæ˜¯ç”·æ€§è¿˜æ˜¯å¥³æ€§ï¼Œå„¿ç«¥æˆ–è€…æœªæˆå¹´æ´»ä¸‹æ¥çš„æ¯”ä¾‹éƒ½æŒºé«˜ã€‚
# - ä»æ€»ä½“æ¥çœ‹ï¼Œå¥³æ€§ç”Ÿå­˜çš„æ¯”ä¾‹è¦è¶…è¿‡ç”·æ€§ï¼Œè¿™æ˜¯æ¯‹åº¸ç½®ç–‘çš„ã€‚
# -Whether it is male or female, the percentage of children or minors surviving is quite high.
# -From an overall point of view, there is no doubt that the proportion of women surviving is higher than that of men.

# In[12]:


g = sns.FacetGrid(data[:891],col="Sex")
g.map_dataframe(sns.pointplot,x="Embarked",y="Survived")
g.add_legend()


# - ç™»èˆ¹æ¸¯å£çš„ç”Ÿå­˜æ¯”ä¾‹æ¯”è¾ƒå¹³ç¼“
# - The survival rate at the embarkation port is relatively flat

# In[13]:


g = sns.FacetGrid(data[:891],col="Embarked")
g.map_dataframe(sns.histplot,x="Age",y="Survived")


# - Cæ¸¯å£å­˜æ´»ä¸‹æ¥çš„äººé›†ä¸­åœ¨20å²å·¦å³ã€‚
# - Sæ¸¯å£æ­»äº¡çš„äººé›†ä¸­åœ¨20å²å·¦å³ã€‚
# - Qæ¸¯å£æ­»äº¡çš„äººé›†ä¸­åœ¨35å²å·¦å³ã€‚
# -The survivors of Port C are concentrated in their 20s.
# -The deaths in Port S are concentrated in their 20s.
# -People who died at Port Q were mostly around 35 years old.

# In[14]:


data["family"] = data["Parch"] + data["SibSp"]
data = data.drop(columns=["Parch","SibSp"])  #delete  Parch and Sibsp
#-------------------------------------------------
g = sns.FacetGrid(data[:891],col="Sex")
g.map_dataframe(sns.pointplot,x="family",y="Survived")


# - å½“ç‹¬è‡ªä¸€ä¸ªäººæ—…æ¸¸æˆ–è€…æºå¸¦å®¶äºº/æœ‹å‹çš„äººæ•°è¶…è¿‡7ä¸ªäººæ—¶ï¼Œç”Ÿå­˜ç‡ä¼šå¾ˆä½ã€‚æ— è®ºæ˜¯ç”·æ€§è¿˜æ˜¯å¥³æ€§ç¾¤ä½“ï¼Œ3ä¸ªäººæ—…æ¸¸çš„ç”Ÿå­˜æ¯”ä¾‹æœ€é«˜ã€‚
# - è¿™è¯´æ˜å®¶äººçš„æ•°é‡ä¹Ÿæ˜¯å­˜æ´»çš„é‡è¦å½±å“å› ç´ ã€‚
# - When traveling alone or when there are more than 7 people with family/friends, the survival rate will be very low. Regardless of whether it is a male or a female group, the three persons have the highest survival rate in tourism.
# - This shows that the number of family members is also an important factor in survival.

# # 2 Feature engineering ğŸ‘
# <div style="color: #fff7f7;
#            display:fill;
#            border-radius:10px;
#            border-style: solid;
#            border-color:#424949;
#            text-align:center;
#            background-color:#003a6c ;
#            font-size:15px;
#            letter-spacing:0.5px;
#            padding: 0.7em;
#            text-align:left">
# <li> Cabinè¯¥åˆ—ç¼ºå¤±æ¯”ä¾‹è¿‡é«˜ï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªé€‰æ‹©ï¼šè€ƒè™‘åˆ é™¤ï¼Œæˆ–è€…å¢åŠ ä¸€ä¸ªæ–°çš„ç‰¹å¾ï¼Œè¡¨ç¤ºæ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼ï¼Œæˆ–è€…åœ¨åŸæœ‰çš„åŸºç¡€ä¸Šï¼Œå°†ç¼ºå¤±å€¼æ ‡è¯†ä¸ºMissã€‚
# <li> Nameåˆ—å¾ˆæœ‰ç”¨ï¼Œä¸€æ–¹é¢æˆ‘ä»¬å¯ä»¥æå–æ¯ä¸ªäººçš„ç§°è°“ï¼Œæ ¹æ®ç§°è°“æ¥å¡«å……å¹´é¾„ç¼ºå¤±å€¼ï¼Œå¦ä¸€æ–¹é¢æå–æ¯ä¸ªäººçš„åå­—ï¼Œè·Ÿåç»­çš„ç¥¨å·/ç¥¨ä»·è”ç³»èµ·æ¥ï¼Œå¯ä»¥ç¡®å®šéƒ¨åˆ†äººçš„å®¶åº­å­˜æ´»æƒ…å†µã€‚
# <li> æˆ‘ä»¬å¯ä»¥è€ƒè™‘å¯¹å¹´é¾„/ç¥¨ä»·è¿›è¡Œåˆ†ç®±ï¼Œè‡³äºé€‰æ‹©åˆ†ç®±çš„ç±»å‹ï¼Œå¯ä»¥å‚è€ƒç­‰å®½/ç­‰é¢‘åˆ†ç®±ã€‚å¦‚æœæ˜¯ä½¿ç”¨è·Ÿè·ç¦»ç›¸å…³çš„ç®—æ³•ï¼Œåˆ†ç®±åä¼šå¾ˆæœ‰ç”¨ï¼Œæˆ–è€…å¯¹ç‰¹å¾è¿›è¡Œç¼©æ”¾ã€‚<hr>
# <li>The percentage of missing values in Cabin column is too high. We have three options: consider deleting, or adding a new feature to indicate whether there are missing values, or mark the missing value as Miss on the original basis.
# <li>The Name column is very useful. On the one hand, we can extract the title of each person and fill in missing age values based on the title. On the other hand, we can extract the name of each person and link it with the subsequent ticket number/fare to determine the survival of some peopleâ€™s families. condition.
# <li>We can consider classifying the age/fare. As for the type of classification, please refer to the equal width/equal frequency classification. If you are using a distance-related algorithm, it will be useful after binning or scaling features.
# </div>

# ### 2.1 å¡«å……ç¼ºå¤±å€¼ Fill in missing values

# In[15]:


from sklearn.impute import KNNImputer
knn = KNNImputer(n_neighbors=3)
data["Age"] = knn.fit_transform(data["Age"].values.reshape(-1,1)).ravel()


# In[16]:


data["Fare"] = data["Fare"].fillna(data[:891]["Fare"].mode()[0])
data["Embarked"] = data["Embarked"].fillna(data[:891]["Embarked"].mode()[0])


# In[17]:


data.head()


# ### 2.2 Name and Ticket---add family_survived

# In[18]:


import re
data["Ticket_num"] = data["Ticket"].apply(lambda x:"".join(re.findall(r"\d+",x)))
data["Name_last"] = data["Name"].apply(lambda x:x.split(",",1)[0])


# In[19]:


#è®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼ï¼Œå¯ä»¥è®¾ç½®ä¸º0.5ï¼Œæˆ–è€…-1ï¼Œåªè¦åŒºåˆ«äº0å’Œ1å³å¯ã€‚ç”±äº0å’Œ1çš„å‡å€¼æ˜¯0.5ï¼Œæ‰€ä»¥æˆ‘ä»¬è®¾ç½®ä¸º0.5
#é€šè¿‡åå­—å’Œç¥¨ä»·ç¡®å®šå®¶åº­
Def = 0.5
data["family_survived"] = Def

#é€šè¿‡åå­—å’Œç¥¨ä»·ç¡®å®šå®¶åº­
da_group = data.groupby(["Name_last","Fare"])
for group,all_data in da_group:
    if (len(all_data) >= 1): #æ˜¯å¦æ˜¯ä¸€ä¸ªå®¶åº­ã€‚Is it a familyï¼Ÿ
            for index, row in all_data.iterrows():
                if (row['family_survived'] == 0) or (row['family_survived']== 0.5):
                    surviv_max = all_data.drop(index)['Survived'].max()
                    surviv_min = all_data.drop(index)['Survived'].min()   #åˆ é™¤æœ¬äººçš„è®°å½•ï¼ŒæŸ¥çœ‹å®¶åº­å…¶ä»–äººçš„ç”Ÿå­˜çŠ¶å†µã€‚Delete my records and view the living conditions of others in the family.
                    ids = row['PassengerId']
                    if (surviv_max == 1.0):
                        data.loc[data['PassengerId'] == ids, 'family_survived'] = 1
                    elif (surviv_min==0.0):
                        data.loc[data['PassengerId'] == ids, 'family_survived'] = 0

print("family survival:", data.loc[data['family_survived'] == 1].shape[0])


# In[20]:


#é€šè¿‡åå­—å’Œç¥¨å·ç¡®å®šå®¶åº­
da_group = data[["PassengerId","Name_last","Survived","family","Fare","Ticket_num"]].groupby(["Name_last","Ticket_num"])
for group,all_data in da_group:
    if (len(all_data) >= 1): #åˆ¤æ–­æ˜¯å¦æ˜¯ä¸€ä¸ªå®¶åº­ã€‚Is it a familyï¼Ÿ
        for index, row in all_data.iterrows():
            surviv_max = all_data.drop(index)['Survived'].max()
            surviv_min = all_data.drop(index)['Survived'].min()   #åˆ é™¤æœ¬äººçš„è®°å½•ï¼ŒæŸ¥çœ‹å®¶åº­å…¶ä»–äººçš„ç”Ÿå­˜çŠ¶å†µã€‚Delete my records and view the living conditions of others in the family.
            ids = row['PassengerId']
            if (surviv_max == 1.0):
                data.loc[data['PassengerId'] == ids, 'family_survived'] = 1
            elif (surviv_min==0.0):
                data.loc[data['PassengerId'] == ids, 'family_survived'] = 0

print("family survival:", data.loc[data['family_survived'] == 1].shape[0])


# In[21]:


#é€šè¿‡ç¥¨å·ç›¸åŒç¡®å®šå®¶åº­
da_group = data.groupby(["Ticket_num"])
for group,all_data in da_group:
    if (len(all_data) >= 1): #æ˜¯å¦æ˜¯ä¸€ä¸ªå®¶åº­ã€‚Is it a familyï¼Ÿ
            for index, row in all_data.iterrows():
                if (row['family_survived'] == 0) or (row['family_survived']== 0.5):
                    surviv_max = all_data.drop(index)['Survived'].max()
                    surviv_min = all_data.drop(index)['Survived'].min()   #åˆ é™¤æœ¬äººçš„è®°å½•ï¼ŒæŸ¥çœ‹å®¶åº­å…¶ä»–äººçš„ç”Ÿå­˜çŠ¶å†µã€‚Delete my records and view the living conditions of others in the family.
                    ids = row['PassengerId']
                    if (surviv_max == 1.0):
                        data.loc[data['PassengerId'] == ids, 'family_survived'] = 1
                    elif (surviv_min==0.0):
                        data.loc[data['PassengerId'] == ids, 'family_survived'] = 0

print("family survival:", data.loc[data['family_survived'] == 1].shape[0])


# In[22]:


data = data.drop(columns=["Name","Ticket","Ticket_num","Name_last"])


# In[23]:


data.head()


# ### 2.3 Age and Fare

# In[24]:


data["Age"] = pd.qcut(data["Age"],7)


# In[25]:


data["Fare"] = pd.qcut(data["Fare"],7)


# In[26]:


data["Age"]


# ### 2.4 Cabin

# In[27]:


data["Cabin"] = data["Cabin"].apply(lambda x:str(x)[0])


# In[28]:


from sklearn.preprocessing import LabelEncoder,StandardScaler
# col = ["Sex","Age","Fare","Embarked","Cabin"]  forå¾ªç¯çš„æ–¹å¼å‡ºç°é”™è¯¯ã€‚æ¢ä¸€ç§åŠæ³•ã€‚
# for i in col:
#     la = LabelEncoder()
#     la.fit(data.loc[:,i].values.reshape(-1, 1))
#     data[i] = la.transform(data.loc[:,i].values.reshape(-1, 1))
la = LabelEncoder()
la.fit(data["Sex"])
data["Sex"] = la.transform(data["Sex"])

la.fit(data["Age"])
data["Age"] = la.transform(data["Age"])

la.fit(data["Fare"])
data["Fare"] = la.transform(data["Fare"])

la.fit(data["Embarked"])
data["Embarked"] = la.transform(data["Embarked"])

la.fit(data["Cabin"])
data["Cabin"] = la.transform(data["Cabin"])

all_col = ["Sex","Age","Fare","Embarked","Pclass","family","family_survived","Cabin"]
for i in all_col:
    std = StandardScaler()
    std.fit(data[:891].loc[:,i].values.reshape(-1, 1)) #traindata
    data[i] = std.transform(data.loc[:,i].values.reshape(-1, 1))


# In[29]:


data.head()


# In[30]:


df_train = data[:891]
df_test = data[891:]


# # 3 Model

# In[31]:


df_train["Survived"].value_counts(normalize=True)


# In[32]:


from sklearn.model_selection import StratifiedKFold,cross_val_score,RandomizedSearchCV,GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,StackingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score,recall_score,precision_score,roc_auc_score
from xgboost import XGBClassifier


# In[33]:


target = df_train.iloc[:,1]
feature = df_train.iloc[:,2:]
stk = StratifiedKFold(n_splits=5,random_state=123,shuffle=True)
for train_index,test_index in stk.split(feature,target):
    x_train,y_train = feature.iloc[train_index],target.iloc[train_index]
    x_test,y_test = feature.iloc[test_index],target.iloc[test_index]


# #### 3.1 GBDT

# In[34]:


#Deault
gbdt = GradientBoostingClassifier(random_state=123)
gbdt.fit(x_train,y_train)
gbdt_pre = gbdt.predict(x_test)
print("gbdt recall:",round(recall_score(y_test,gbdt_pre),2))
print("gbdt precision:",round(precision_score(y_test,gbdt_pre),2))
print("gbdt f1_score:",round(f1_score(y_test,gbdt_pre),2))
print("gbdt rou_auc_score:",round(roc_auc_score(y_test,gbdt_pre),2))


# In[35]:


params = {
    
    "max_depth":(2,8),
    "learning_rate":(0.01,0.1),
    "min_samples_leaf":(1,3),
    "n_estimators":(100,300)
}
gbdt_gd = RandomizedSearchCV(gbdt,param_distributions=params,cv=5,scoring="roc_auc")
gbdt_gd.fit(x_train,y_train)
print(gbdt_gd.best_score_)
print(gbdt_gd.best_estimator_)
print(gbdt_gd.best_params_)


# In[36]:


gbdt_pre = gbdt_gd.predict(x_test)
print("gbdt_gd recall:",round(recall_score(y_test,gbdt_pre),2))
print("gbdt_gd precision:",round(precision_score(y_test,gbdt_pre),2))
print("gbdt_gd f1_score:",round(f1_score(y_test,gbdt_pre),2))
print("gbdt_gd rou_auc_score:",round(roc_auc_score(y_test,gbdt_pre),2))


# #### 3.2 Adaboost

# In[37]:


#Deault
adaboost = AdaBoostClassifier(random_state=123)
adaboost.fit(x_train,y_train)
adaboost_pre = gbdt.predict(x_test)
print("adaboost recall:",round(recall_score(y_test,adaboost_pre),2))
print("adaboost precision:",round(precision_score(y_test,adaboost_pre),2))
print("adaboost f1_score:",round(f1_score(y_test,adaboost_pre),2))
print("adaboost rou_auc_score:",round(roc_auc_score(y_test,adaboost_pre),2))


# In[38]:


params = {
    
    "learning_rate":(0.01,0.2),
    "n_estimators":(50,300)
}
ada_gd = RandomizedSearchCV(adaboost,param_distributions=params,cv=5,scoring="roc_auc")
ada_gd.fit(x_train,y_train)
print(ada_gd.best_score_)
print(ada_gd.best_estimator_)
print(ada_gd.best_params_)


# In[39]:


ada_pre = ada_gd.predict(x_test)
print("ada_gd recall:",round(recall_score(y_test,ada_pre),2))
print("ada_gd precision:",round(precision_score(y_test,ada_pre),2))
print("ada_gd f1_score:",round(f1_score(y_test,ada_pre),2))
print("ada_gd rou_auc_score:",round(roc_auc_score(y_test,ada_pre),2))


# ### 3.3 RF

# In[40]:


rf = RandomForestClassifier(random_state=123)
rf.fit(x_train,y_train)
rf_pre = rf.predict(x_test)
print("rf recall:",round(recall_score(y_test,rf_pre),2))
print("rf precision:",round(precision_score(y_test,rf_pre),2))
print("rf f1_score:",round(f1_score(y_test,rf_pre),2))
print("rf rou_auc_score:",round(roc_auc_score(y_test,rf_pre),2))


# In[41]:


params = {
    "max_depth":(3,10),
    "n_estimators":(50,300),
    "min_samples_leaf":(1,5),
    "min_samples_split":(0.1,0.2),
    "min_impurity_decrease":(0,0.2),
}
rf_gd = RandomizedSearchCV(rf,param_distributions=params,cv=5,scoring="roc_auc")
rf_gd.fit(x_train,y_train)
print(rf_gd.best_score_)
print(rf_gd.best_estimator_)
print(rf_gd.best_params_)


# In[42]:


rfgd_pre = rf_gd.predict(x_test)
print("rf_gd recall:",round(recall_score(y_test,rfgd_pre),2))
print("rf_gd precision:",round(precision_score(y_test,rfgd_pre),2))
print("rf_gd f1_score:",round(f1_score(y_test,rfgd_pre),2))
print("rf_gd rou_auc_score:",round(roc_auc_score(y_test,rfgd_pre),2))


# ### 3.4 KNN

# In[43]:


knn = KNeighborsClassifier()
knn.fit(x_train,y_train)
knn_pre = knn.predict(x_test)
print("knn recall:",round(recall_score(y_test,knn_pre),2))
print("knn precision:",round(precision_score(y_test,knn_pre),2))
print("knn f1_score:",round(f1_score(y_test,knn_pre),2))
print("knn rou_auc_score:",round(roc_auc_score(y_test,knn_pre),2))


# In[44]:


params = {'algorithm': ['auto'], 'weights': ['uniform', 'distance'], 'leaf_size': list(range(1,50,5)), 
               'n_neighbors': [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20,22]}
knn_gd=GridSearchCV(knn, param_grid = params, verbose=True, 
                cv=10, scoring = "roc_auc")
knn_gd.fit(x_train, y_train)
print(knn_gd.best_score_)
print(knn_gd.best_estimator_)
print(knn_gd.best_params_)


# In[45]:


knngd_pre = knn_gd.predict(x_test)
print("knngd_pre recall:",round(recall_score(y_test,knngd_pre),2))
print("knngd_pre precision:",round(precision_score(y_test,knngd_pre),2))
print("knngd_pre f1_score:",round(f1_score(y_test,knngd_pre),2))
print("knngd_pre rou_auc_score:",round(roc_auc_score(y_test,knngd_pre),2))


# ### 3.5 XGboost

# In[46]:


#Default
xgb = XGBClassifier(random_state=123,eval_metric="logloss",use_label_encoder=False)
xgb.fit(x_train,y_train)
xgb_pre = xgb.predict(x_test)
print("xgb recall:",round(recall_score(y_test,xgb_pre),2))
print("xgb precision:",round(precision_score(y_test,xgb_pre),2))
print("xgb f1_score:",round(f1_score(y_test,xgb_pre),2))
print("xgb rou_auc_score:",round(roc_auc_score(y_test,xgb_pre),2))


# In[47]:


params ={
    "max_depth":(2,8),
    "n_estimators":(100,400),
    "learning_rate":(0.5,1),
    "gamma":(0,0.2),
    "reg_lambda":(0.1,0.3),
    "reg_alpha":(0.1,0.3),
    "colsample_bytree":(0.8,1),
}

xgb_gd = RandomizedSearchCV(xgb,param_distributions=params,cv=5,scoring="roc_auc")
xgb_gd.fit(x_train,y_train)
print(xgb_gd.best_score_)
print(xgb_gd.best_estimator_)
print(xgb_gd.best_params_)


# In[48]:


xgbgd_pre = xgb_gd.predict(x_test)
print("xgb_gd recall:",round(recall_score(y_test,xgbgd_pre),2))
print("xgb_gd precision:",round(precision_score(y_test,xgbgd_pre),2))
print("xgb_gd f1_score:",round(f1_score(y_test,xgbgd_pre),2))
print("xgb_gd rou_auc_score:",round(roc_auc_score(y_test,xgbgd_pre),2))


# ### 3.6 Logistic

# In[49]:


logstic = LogisticRegression(random_state=123)
logstic.fit(x_train,y_train)
logstic_pre = logstic.predict(x_test)
print("logstic recall:",round(recall_score(y_test,logstic_pre),2))
print("logstic precision:",round(precision_score(y_test,logstic_pre),2))
print("logstic f1_score:",round(f1_score(y_test,logstic_pre),2))
print("logstic rou_auc_score:",round(roc_auc_score(y_test,logstic_pre),2))


# In[50]:


prams = {
    
    "C":[0.1,0.2,0.3,0.4,0.5],
    "solver":["newton-cg","lbfgs","sag","saga"]
}

logstic_gd=GridSearchCV(logstic, param_grid = prams, verbose=True, 
                cv=10, scoring = "roc_auc")
logstic_gd.fit(x_train, y_train)
print(logstic_gd.best_score_)
print(logstic_gd.best_estimator_)
print(logstic_gd.best_params_)


# In[51]:


logsticgd_pre = logstic_gd.predict(x_test)
print("logstic recall:",round(recall_score(y_test,logsticgd_pre),2))
print("logstic precision:",round(precision_score(y_test,logsticgd_pre),2))
print("logstic f1_score:",round(f1_score(y_test,logsticgd_pre),2))
print("logstic rou_auc_score:",round(roc_auc_score(y_test,logsticgd_pre),2))


# In[52]:


df_t = df_test.iloc[:,2:]
ids = df_test.iloc[:,0]
df_t


# - soft VotingClassifier

# In[53]:


# from sklearn.ensemble import VotingClassifier
# esti = [("gbdt",gbdt_gd),("rf",rf_gd),("adaboost",ada_gd),("knn",knn_gd),("xgb",xgb_gd)]
# softvoting = VotingClassifier(estimators=esti,voting="soft")
# softvoting.fit(x_train, y_train)
# softvoting_pre = softvoting.predict(x_test)
# print("softvoting recall:",round(recall_score(y_test,softvoting_pre),2))
# print("softvoting precision:",round(precision_score(y_test,softvoting_pre),2))
# print("softvoting f1_score:",round(f1_score(y_test,softvoting_pre),2))
# print("softvoting rou_auc_score:",round(roc_auc_score(y_test,softvoting_pre),2))


# - hard VotingClassifier

# In[54]:


# voting = VotingClassifier(estimators=esti,voting="hard")
# voting.fit(x_train, y_train)
# voting_pre = voting.predict(x_test)
# print("hard voting recall:",round(recall_score(y_test,voting_pre),2))
# print("hard voting precision:",round(precision_score(y_test,voting_pre),2))
# print("hard voting f1_score:",round(f1_score(y_test,voting_pre),2))
# print("hard voting rou_auc_score:",round(roc_auc_score(y_test,voting_pre),2))

#ç¡¬æŠ•ç¥¨çš„æ•ˆæœä¸å¦‚è½¯æŠ•ç¥¨


# - Stacking

# In[55]:


# esti = [("gbdt",gbdt_gd),("rf",rf_gd),("adaboost",ada_gd),("knn",knn_gd),("xgb",xgb_gd)]
# stack = StackingClassifier(estimators=esti,final_estimator=logstic_gd,cv=5)
# stack.fit(x_train, y_train)
# stack_pre = voting.predict(x_test)
# print("stack recall:",round(recall_score(y_test,stack_pre),2))
# print("stack precision:",round(precision_score(y_test,stack_pre),2))
# print("stack f1_score:",round(f1_score(y_test,stack_pre),2))
# print("stack rou_auc_score:",round(roc_auc_score(y_test,stack_pre),2))

#Stackä¸å¦‚è½¯æŠ•ç¥¨


# In[56]:


sub = pd.DataFrame()
sub["PassengerId"] = ids
sub["Survived"] = knn_gd.predict(df_t)
sub["Survived"] = sub["Survived"].astype(int)
sub


# In[57]:


sub.to_csv("submission.csv",index=None)
print("End!")


# <div style="color: #fff7f7;
#            display:fill;
#            border-radius:10px;
#            border-style: solid;
#            border-color:#424949;
#            text-align:center;
#            background-color:#003a6c ;
#            font-size:15px;
#            letter-spacing:0.5px;
#            padding: 0.7em;
#            text-align:left">
#     <h1 style="text-align:center;font-weight: 20px; color:White;">
#        ResultğŸ§</h1>
#     <li>å½“æˆ‘åˆ é™¤Cabinæ—¶ï¼Œæäº¤çš„æ¨¡å‹åˆ†æ•°æ˜¯0.79ï¼Œå½“æˆ‘åŠ å…¥Cabinæ—¶ï¼Œå°†ç¼ºå¤±å€¼è¡¨ç¤ºä¸ºN,æäº¤çš„æ¨¡å‹åˆ†æ•°è¾¾åˆ°äº†0.80382ã€‚
#     <li>å½“æˆ‘å°†å¹´é¾„å’Œç¥¨ä»·åˆ†ç®±è°ƒæ•´ä¸º5ä¸ªæ—¶ï¼Œæ¨¡å‹åˆ†æ•°ä¸‹é™åˆ°0.78ã€‚ç„¶è€Œå½“æˆ‘è°ƒæ•´ä¸º7ä¸ªåˆ†ç®±æ—¶ï¼Œæ¨¡å‹åˆ†æ•°è¾¾åˆ°0.80382ã€‚
#     <li>è½¯æŠ•ç¥¨ä»¥åŠç¡¬æŠ•ç¥¨è¿˜æœ‰æ¨¡å‹å †å Stackçš„æ–¹å¼ï¼Œæ•ˆæœä¸å¦‚KNNï¼Œè½¯æŠ•ç¥¨ç­‰æ–¹å¼è¾¾åˆ°çš„åˆ†æ•°æ˜¯0.78ã€‚ä½†æ˜¯åœ¨è½¯æŠ•ç¥¨ä¸­åˆ é™¤é€»è¾‘å›å½’çš„æ¨¡å‹ï¼Œæäº¤çš„åˆ†æ•°å¯ä»¥è¾¾åˆ°0.80143ã€‚<hr>
#     <li>When I deleted Cabin, the submitted model score was 0.79. When I joined Cabin, the missing value was expressed as N, and the submitted model score reached 0.80382.
#     <li>When I adjusted the age and fare bins to 5, the model score dropped to 0.78. However, when I adjusted to 7 bins, the model score reached 0.80382.
#     <li>Soft voting and hard voting, as well as the way of stacking models, are not as effective as KNN, and the score achieved by soft voting is 0.78. But if the logistic regression model is deleted in the soft voting, the submitted score can reach 0.80143.<hr>
#     å¯èƒ½éœ€è¦æ”¹è¿›çš„åœ°æ–¹ï¼š
# <li>é’ˆå¯¹å¹´é¾„ç¼ºå¤±å€¼å¡«å……ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘ä½¿ç”¨é˜¶çº§ç‰¹å¾æˆ–è€…æå–åå­—ç‰¹å¾ä¸­çš„ç§°è°“ï¼Œå¯¹å¹´é¾„æ¥è¿›è¡Œç¼ºå¤±å€¼å¡«å……ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æš´åŠ›çš„KNNå¡«è¡¥çš„æ–¹å¼ã€‚
# <li>å®¶åº­æ­»äº¡ç‰¹å¾ï¼šæˆ‘ä»¬å¯ä»¥ä¸ä»…å¯ä»¥é€šè¿‡Ticketå’ŒFareä»¥åŠName-lastï¼Œæˆ–è®¸æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨Cabinæ¥ç»¼åˆåˆ¤æ–­ï¼Œæ¯•ç«Ÿç›¸ä¼´å‡ºæ¸¸çš„å®¶åº­ï¼Œè‚¯å®šä¹Ÿä¼šä»ç›¸åŒçš„Embarkedç™»èˆ¹ã€‚<hr>
# Possible areas for improvement:
# <li> For the filling of missing values for age, we can consider using class features or extracting titles from name features to fill in missing values for age instead of using violent KNN filling.
# <li>Family death characteristics: We can not only use Ticket, Fare and Name-last, but perhaps we can also use Cabin to make a comprehensive judgment. After all, families traveling with us will definitely board the same Embarked.
# </div>
