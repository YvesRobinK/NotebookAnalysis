#!/usr/bin/env python
# coding: utf-8

# In[1]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# ## Loading the data

# In[2]:


df_train = pd.read_csv('/kaggle/input/playground-series-s3e17/train.csv')
df_train


# In[3]:


df_test = pd.read_csv('/kaggle/input/playground-series-s3e17/test.csv')
df_test


# For XGBoost columns shouldn't have brackets and other signs so that is removed below

# In[4]:


import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
df_train.columns = [col.replace('[', '').replace(']', '').replace('<', '') if col not in ['Type'] else col for col in df_train.columns]
df_test.columns = [col.replace('[', '').replace(']', '').replace('<', '') if col not in ['Type'] else col for col in df_test.columns]
df_train.columns


# ## Feature Engineering

# The feature engineering process involves transforming and creating new features from existing data to improve the performance of machine learning models. In the below code snippet, four main feature engineering techniques are applied.
# 
# Statistical aggregations, such as mean and maximum values, are calculated for specific groups (Product ID) to capture summary information about Torque and Tool wear.
# 
# Lagged values are created by shifting Torque and Tool wear variables by one time step, enabling the model to consider the previous values of these features.
# 
# Ratio features are generated by dividing Torque by Rotational speed and Air temperature by Process temperature, providing insights into the relationships between these variables.
# 
# Rolling window statistics, including rolling mean and standard deviation, are computed on Torque to capture recent trends and variations in the data.

# In[5]:


# Statistical Aggregations
df_train['TorqueMean'] = df_train.groupby('Product ID')['Torque Nm'].transform('mean')
df_train['ToolWearMax'] = df_train.groupby('Product ID')['Tool wear min'].transform('max')

# Time Series Features (Lagged Values)
df_train['TorqueLag1'] = df_train.groupby('Product ID')['Torque Nm'].shift(1)
df_train['ToolWearLag1'] = df_train.groupby('Product ID')['Tool wear min'].shift(1)

# Ratio Features
df_train['TorqueToSpeedRatio'] = df_train['Torque Nm'] / df_train['Rotational speed rpm']
df_train['TempRatio'] = df_train['Air temperature K'] / df_train['Process temperature K']

# Rolling Window Statistics
df_train['TorqueRollingMean'] = df_train.groupby('Product ID')['Torque Nm'].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)


# In[6]:


# Statistical Aggregations
df_test['TorqueMean'] = df_test.groupby('Product ID')['Torque Nm'].transform('mean')
df_test['ToolWearMax'] = df_test.groupby('Product ID')['Tool wear min'].transform('max')

# Time Series Features (Lagged Values)
df_test['TorqueLag1'] = df_test.groupby('Product ID')['Torque Nm'].shift(1)
df_test['ToolWearLag1'] = df_test.groupby('Product ID')['Tool wear min'].shift(1)

# Ratio Features
df_test['TorqueToSpeedRatio'] = df_test['Torque Nm'] / df_test['Rotational speed rpm']
df_test['TempRatio'] = df_test['Air temperature K'] / df_test['Process temperature K']

# Rolling Window Statistics
df_test['TorqueRollingMean'] = df_test.groupby('Product ID')['Torque Nm'].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)



# In[7]:


df_train

df_test
# In[8]:


df_s= pd.get_dummies(df_train['Type'])
df_train= df_train.drop('Type', axis =1)
df_train = pd.concat([df_train, df_s], axis =1)
# Separate the features (X) and the target variable (y)
y = df_train['Machine failure']
X = df_train.drop(['id', 'Product ID', 'Machine failure'], axis=1)



# Split the data into training set and validation set
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)


# The code below performs one-hot encoding on the 'Type' column of the 'df_train' dataframe 
# and adds the resulting dummy variables to the dataframe. Then, it separates the target 
# variable 'Machine failure' from the features and prepares the data for preprocessing 
# and splitting into training and validation sets.

# ## Model

# Logistic regression was tried initially which gave a lesser score

# In[9]:


'''model = LogisticRegression()
# Define the hyperparameters to search over
param_grid = {
    'C': [0.01, 0.1, 1.0, 10.0],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}'''


# The code below performs hyperparameter tuning for an XGBoost regressor by defining a parameter grid, initializing the regressor, and using GridSearchCV to find the best hyperparameters. It then prints the best hyperparameters, obtains the best model, and validates it by making predictions on the validation set.

# In[10]:


import xgboost as xgb

# Hyperparameter Tuning
# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5]
    
}


# Initialize the XGBoost regressor
model = xgb.XGBRegressor(objective='reg:logistic')

# Perform grid search with cross-validation
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters found
print("Best Hyperparameters: ", grid_search.best_params_)

# Get the best model
best_model = grid_search.best_estimator_

# Validate the model
y_pred = best_model.predict(X_val)



# ## Predictions on test data

# In[11]:


df_s_t= pd.get_dummies(df_test['Type'])
df_test= df_test.drop('Type', axis =1)
df_test = pd.concat([df_test, df_s_t], axis =1)
new_X = df_test.drop(['id', 'Product ID'], axis=1)
probabilities = best_model.predict(new_X)# Probability of machine failure

print(probabilities)


# In[12]:


sub = pd.read_csv('/kaggle/input/playground-series-s3e17/sample_submission.csv')
sub


# In[13]:


sub['Machine failure'] = probabilities
sub


# In[14]:


sub.to_csv('submission.csv', index = False )

