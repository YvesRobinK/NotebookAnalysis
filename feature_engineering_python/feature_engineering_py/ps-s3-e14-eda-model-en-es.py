#!/usr/bin/env python
# coding: utf-8

# # <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#37FABC; font-size:160%; text-align:center;padding: 0px; border-bottom: 5px solid #407A68">PlayGround Series S3 E14 EDA and simple model</p>

# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 5px solid #008F77">Intro</p>
# 
# **üü¶EN**:
# <div class="alert alert-block alert-info" style="font-size:14px; font-family:verdana; line-height: 1.7em; color:#5361fc;">
# This Kaggle workbook aims to provide a comprehensive exploratory data analysis (EDA) and a set of simple models (which will not be optimized), but which can give a vague idea of how to choose the best model for the given data set, with the ultimate goal of making decisions.
# Through this EDA, we will be able to get a deeper understanding of the structure of the data, the values that have a relationship between them and the missing values and pattern or outliers that may affect when performing the modeling or selecting the model we want to use for prediction/recommendation. By performing an EDA, we can identify potential pitfalls and make the decisions and subsequent processing necessary to improve the performance and accuracy of the models.
# </div>
# 
# **üü•ES**: 
# <div class="alert alert-block alert-info" style="font-size:14px; font-family:verdana; line-height: 1.7em; background-color: #c9b1fa; color:#38196e;">
# Este cuaderno Kaggle tiene el objetivo proporcionar un an√°lisis exploratorio de datos (AED) exhaustivo y un conjunto de modelos simples (los cuales no estar√°n optimizados), pero que pueden llegar a dar una vaga idea para escoger el mejor modelo, para el conjunto de datos dado, con el objetivo final de tomar decisiones.
# 
# A trav√©s de este AED, podremos obtener una comprensi√≥n m√°s profunda de la estructura de los datos, los valores que tiene una relaci√≥n entre ellos y los valores que faltan y patr√≥n o valores an√≥malos que pueda afectar a la hora de realizar el modelado o seleccionar el modelo que queremos utilizar para la predicci√≥n / recomendaci√≥n. Al realizar un EDA, podemos identificar posibles obst√°culos y tomar las decisiones, y posteriormente el procesado necesario para mejorar el rendimiento y la precisi√≥n de los modelos.
# </div>
# 
# Original model: https://www.kaggle.com/code/paddykb/ps-s3e14-flaml-bfi-be-bop-a-blueberry-do-dah Author: [@paddykb](https://www.kaggle.com/paddykb)

# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 3px solid #008F77">Data information</p>
# 
# **üü¶EN**:
# The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. (Since this is Playground 3.14, it seems like we need a Blueberry Pie joke here?) Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
# 
# Files
# * ```train.csv``` - the training dataset; yield is the target
# * ```test.csv```- the test dataset; your objective is to predict the yield given the other features
# * ```sample_submission.csv``` - a sample submission file in the correct format
# 
# 
# *Context*
# 
# Blueberries are perennial flowering plants with blue or purple berries. They are classified in the section Cyanococcus within the genus Vaccinium. Vaccinium also includes cranberries, bilberries, huckleberries, and Madeira blueberries. Commercial blueberries‚Äîboth wild (lowbush) and cultivated (highbush)‚Äîare all native to North America. The highbush varieties were introduced into Europe during the 1930s.
# 
# Blueberries are usually prostrate shrubs that can vary in size from 10 centimeters (4 inches) to 4 meters (13 feet) in height. In the commercial production of blueberries, the species with small, pea-size berries growing on low-level bushes are known as "lowbush blueberries" (synonymous with "wild"), while the species with larger berries growing on taller, cultivated bushes are known as "highbush blueberries". Canada is the leading producer of lowbush blueberries, while the United States produces some 40% of the world s supply of highbush blueberries.
# 
# *Content*
# 
# "The dataset used for predictive modeling was generated by the Wild Blueberry Pollination Simulation Model, which is an open-source, spatially-explicit computer simulation program that enables exploration of how various factors, including plant spatial arrangement, outcrossing and self-pollination, bee species compositions and weather conditions, in isolation and combination, affect pollination efficiency and yield of the wild blueberry agroecosystem. The simulation model has been validated by the field observation and experimental data collected in Maine USA and Canadian Maritimes during the last 30 years and now is a useful tool for hypothesis testing and theory development for wild blueberry pollination researches."
# 
# *Features Unit Description*
# 
# Clonesize m2 The average blueberry clone size in the field
# Honeybee bees/m2/min Honeybee density in the field
# Bumbles bees/m2/min Bumblebee density in the field
# Andrena bees/m2/min Andrena bee density in the field
# Osmia bees/m2/min Osmia bee density in the field
# MaxOfUpperTRange ‚ÑÉ The highest record of the upper band daily air temperature during the bloom season
# MinOfUpperTRange ‚ÑÉ The lowest record of the upper band daily air temperature
# AverageOfUpperTRange ‚ÑÉ The average of the upper band daily air temperature
# MaxOfLowerTRange ‚ÑÉ The highest record of the lower band daily air temperature
# MinOfLowerTRange ‚ÑÉ The lowest record of the lower band daily air temperature
# AverageOfLowerTRange ‚ÑÉ The average of the lower band daily air temperature
# RainingDays Day The total number of days during the bloom season, each of which has precipitation larger than zero
# AverageRainingDays Day The average of raining days of the entire bloom season
# 
# 
# **üü•ES**:
# El conjunto de datos para esta competici√≥n (tanto de entrenamiento como de prueba) se gener√≥ a partir de un modelo de aprendizaje profundo entrenado en el conjunto de datos de predicci√≥n de rendimiento de ar√°ndanos silvestres. (Dado que se trata de Playground 3.14, parece que necesitamos un chiste sobre la tarta de ar√°ndanos). Las distribuciones de las caracter√≠sticas son parecidas, aunque no exactamente iguales, a las del original. Si√©ntase libre de utilizar el conjunto de datos original como parte de esta competici√≥n, tanto para explorar las diferencias como para ver si la incorporaci√≥n del original en el entrenamiento mejora el rendimiento del modelo.
# 
# Archivos
# * ```train.csv``` - el conjunto de datos de entrenamiento; el rendimiento es el objetivo
# * ```test.csv``` - el conjunto de datos de prueba; su objetivo es predecir el rendimiento dadas las otras caracter√≠sticas
# * ```sample_submission.csv``` - un archivo de env√≠o de muestra en el formato correcto
# 
# 
# *Contexto*
# 
# Los ar√°ndanos son plantas perennes con flores y bayas de color azul o p√∫rpura. Se clasifican en la secci√≥n Cyanococcus dentro del g√©nero Vaccinium. Vaccinium tambi√©n incluye los ar√°ndanos rojos, los ar√°ndanos rojos, los ar√°ndanos silvestres y los ar√°ndanos de Madeira. Los ar√°ndanos comerciales, tanto silvestres (lowbush) como cultivados (highbush), son originarios de Norteam√©rica. Las variedades highbush se introdujeron en Europa en la d√©cada de 1930.
# 
# Los ar√°ndanos suelen ser arbustos postrados cuyo tama√±o puede variar entre 10 cent√≠metros (4 pulgadas) y 4 metros (13 pies) de altura. En la producci√≥n comercial de ar√°ndanos, las especies con bayas peque√±as, del tama√±o de un guisante, que crecen en arbustos bajos se conocen como "ar√°ndanos lowbush" (sin√≥nimo de "silvestres"), mientras que las especies con bayas m√°s grandes que crecen en arbustos m√°s altos y cultivados se conocen como "ar√°ndanos highbush". Canad√° es el principal productor de ar√°ndanos "lowbush", mientras que Estados Unidos produce alrededor del 40% de los ar√°ndanos "highbush" del mundo.
# 
# *Contenido*
# 
# "El modelo de simulaci√≥n de la polinizaci√≥n de ar√°ndanos silvestres es un programa inform√°tico de simulaci√≥n de c√≥digo abierto y espacialmente expl√≠cito que permite explorar c√≥mo diversos factores, entre ellos la disposici√≥n espacial de las plantas, el entrecruzamiento y la autopolinizaci√≥n, la composici√≥n de las especies de abejas y las condiciones meteorol√≥gicas, de forma aislada y combinada, afectan a la eficacia de la polinizaci√≥n y al rendimiento del agroecosistema de los ar√°ndanos silvestres. El modelo de simulaci√≥n ha sido validado por la observaci√≥n de campo y los datos experimentales recogidos en Maine (EE.UU.) y las provincias mar√≠timas canadienses durante los √∫ltimos 30 a√±os, y ahora es una herramienta √∫til para la comprobaci√≥n de hip√≥tesis y el desarrollo de teor√≠as para la investigaci√≥n de la polinizaci√≥n del ar√°ndano silvestre."
# 
# *Caracter√≠sticas Unidad Descripci√≥n-*
# 
# Clonesize m2 Tama√±o medio de los clones de ar√°ndanos en el campo
# Honeybee bees/m2/min Densidad de abejas en el campo
# Bumbles bees/m2/min Densidad de abejorros en el campo
# Abejas Andrena/m2/min Densidad de abejas Andrena en el campo
# Abejas Osmia/m2/min Densidad de abejas Osmia en el campo
# MaxOfUpperTRange ‚ÑÉ El registro m√°s alto de la temperatura diaria del aire en la banda superior durante la temporada de floraci√≥n
# MinOfUpperTRange ‚ÑÉ El registro m√°s bajo de la temperatura diaria del aire en la banda superior
# AverageOfUpperTRange ‚ÑÉ La media de la temperatura diaria del aire en la banda superior
# MaxOfLowerTRange ‚ÑÉ El registro m√°s alto de la temperatura diaria del aire en la banda inferior
# MinOfLowerTRange ‚ÑÉ El registro m√°s bajo de la temperatura diaria del aire en la banda inferior
# AverageOfLowerTRange ‚ÑÉ Media de la temperatura diaria del aire en la banda inferior
# RainingDays Day El n√∫mero total de d√≠as durante la temporada de floraci√≥n, cada uno de los cuales tiene precipitaciones mayores que cero
# AverageRainingDays D√≠a Media de los d√≠as de lluvia de toda la temporada de floraci√≥n
# 
# 

# In[1]:


get_ipython().run_cell_magic('capture', '', '!pip install flaml\nfrom flaml import AutoML\n!pip install sklego\n')


# In[2]:


import os 
import sys
import math
import time
import warnings
import numpy as np 
import pandas as pd
import seaborn as sns
import lightgbm as lgb
import plotly.express as px
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import matplotlib.colors as mcolors



from lightgbm import LGBMRegressor
from scipy.cluster import hierarchy
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from catboost import CatBoostRegressor
from sklearn.svm import SVR, LinearSVR
from sklego.linear_model import LADRegression
from sklearn.compose import ColumnTransformer
from scipy.spatial.distance import squareform
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor, XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.cross_decomposition import PLSRegression
from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from scipy.cluster.hierarchy import dendrogram, ward, set_link_color_palette
from sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor, StackingRegressor
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor

warnings.filterwarnings('ignore')


# In[3]:


# Put theme of notebook 
from colorama import Fore, Style

# Colors
red = Fore.RED + Style.BRIGHT
mgta = Fore.MAGENTA + Style.BRIGHT
yllw = Fore.YELLOW + Style.BRIGHT
cyn = Fore.CYAN + Style.BRIGHT
blue = Fore.BLUE + Style.BRIGHT

# Reset
res = Style.RESET_ALL


# In[4]:


# Colors
YELLOW = "#F7C53E"

CYAN_G = "#0CF7AF"
CYAB_DARK = "#11AB7C"

PURPLE = "#D826F8"
PURPLE_DARJ = "#9309AB"
PURPLE_L = "#b683d6"

BLUE = "#0C97FA"
RED = "#FA1D19"
ORANGE = "#FA9F19"
GREEN = "#0CFA58"
LIGTH_BLUE = "#01FADC"
S_BLUE = "#81c9e6"
DARK_BLUE = "#394be6"
# Palettes
PALETTE_2 = [CYAN_G, PURPLE]
PALETTE_3 = [YELLOW, CYAN_G, PURPLE]
PALETTE_4 = [YELLOW, ORANGE, PURPLE, LIGTH_BLUE]
PALETTE_5 = [PURPLE_DARJ, PURPLE_L, PURPLE, BLUE, LIGTH_BLUE]
PALETTE_6 = [BLUE, RED, ORANGE, GREEN, LIGTH_BLUE, PURPLE]

# Vaporwave palette by Francesc Oliveras
PALETTE_7 = [PURPLE_DARJ, PURPLE_L, PURPLE, BLUE, LIGTH_BLUE, DARK_BLUE, S_BLUE]
PALETTE_7_C = [PURPLE_DARJ, BLUE, PURPLE, LIGTH_BLUE, PURPLE_L, S_BLUE, DARK_BLUE]
INCLUDE_ORIGINAL = True
RANDOM_STATE = 500

sns.palplot(sns.color_palette(PALETTE_7))

# Set Style
sns.set_style("whitegrid")
sns.despine(left=True, bottom=True)

cmap = mcolors.LinearSegmentedColormap.from_list("", PALETTE_2)
cmap_2 = mcolors.LinearSegmentedColormap.from_list("", [S_BLUE, PURPLE_DARJ])

font_family = dict(layout=go.Layout(font=dict(family="Franklin Gothic", size=10), width=1000, height=500))
SEED = 500

SPLITS = 5
k = KFold(n_splits = SPLITS, random_state = SEED, shuffle = True)


# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 3px solid #008F77">Constants</p>

# In[5]:


PATH = "/kaggle/input/playground-series-s3e14"
TRAIN_FILENAME = "train.csv"
TEST_FILENAME = "test.csv"
SUBMISSION_FILENAME = "sample_submission.csv"
ORIGINAL_PATH = "/kaggle/input/wild-blueberry-yield-prediction-dataset/WildBlueberryPollinationSimulationData.csv"
N_OUTER_FOLDS = 9
N_FOLDS = 10
N_REPEATS = 10


FIRST_TIME_BUDGET = 1 #1200  # Exploration Budget for first fit
MIN_TIME_BUDGET = 1 ##10     # subseqent fit times are reduced

# Data dir
TEST_DIR = os.path.join(PATH, TEST_FILENAME)
TRAIN_DIR = os.path.join(PATH, TRAIN_FILENAME)
SUBMISSION_DIR = os.path.join(PATH, SUBMISSION_FILENAME)


# In[6]:


ID = "id"
CLONESIZE = "clonesize"
HONEYBEE = "honeybee"
BUMBLES = "bumbles"
ANDRENA = "andrena"
OSMIA = "osmia"
MAXOFUPPERRANGE = "MaxOfUpperTRange"
MINOFUPPERRANGE = "MinOfUpperTRange"
AVERAGEOFUPPERRANGE = "AverageOfUpperTRange"
MAXOFLOWERRANGE = "MaxOfLowerTRange"
MINOFLOWERRANGE = "MinOfLowerTRange"
AVERAGEOFLOWERRANGE = "AverageOfLowerTRange"
RAININGDAYS = "RainingDays"
AVERAGERAININGDAYS = "AverageRainingDays"
FRUITSET = "fruitset"
FRUITMASS = "fruitmass"
SEEDS = "seeds"
TARGET = "yield"


# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 3px solid #008F77">Functions</p>

# In[7]:


def show_corr_heatmap(df, title):
    
    corr = df.corr()
    mask = np.zeros_like(corr)
    mask[np.triu_indices_from(mask)] = True

    plt.figure(figsize = (15, 10))
    plt.title(title)
    sns.heatmap(corr, annot = True, linewidths=.5, fmt=".2f", square=True, mask = mask, cmap=cmap_2)
    plt.show()


# In[8]:


def data_description(df):
    print("Data description")
    print(f"Total number of records {df.shape[0]}")
    print(f'number of features {df.shape[1]}\n\n')
    columns = df.columns
    data_type = []
    
    # Get the datatype of features
    for col in df.columns:
        data_type.append(df[col].dtype)
        
    n_uni = df.nunique()
    # Number of NaN values
    n_miss = df.isna().sum()
    
    names = list(zip(columns, data_type, n_uni, n_miss))
    variable_desc = pd.DataFrame(names, columns=["Name","Type","Unique levels","Missing"])
    print(variable_desc)


# In[9]:


def cross_val_score(model, cv = k, label = ''):
    
    X = train_df.copy()
    y = X.pop('yield')
    
    #initiate prediction arrays and score lists
    val_predictions = np.zeros((len(train_df)))
    train_predictions = np.zeros((len(train_df)))
    train_mae, val_mae = [], []
    
    #training model, predicting prognosis probability, and evaluating log loss
    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):
        
        model.fit(X.iloc[train_idx], y.iloc[train_idx])

        train_preds = model.predict(X.iloc[train_idx])
        val_preds = model.predict(X.iloc[val_idx])
                  
        train_predictions[train_idx] += train_preds
        val_predictions[val_idx] += val_preds
        
        train_score = mean_absolute_error(y.iloc[train_idx], train_preds)
        validation_sc = mean_absolute_error(y.iloc[val_idx], val_preds)
        
        train_mae.append(train_score)
        val_mae.append(validation_sc)
    
    print(f'Val MAE: {np.mean(val_mae):.5f} ¬± {np.std(val_mae):.5f} | Train MAE: {np.mean(train_mae):.5f} ¬± {np.std(train_mae):.5f} | {label}')
    
    return val_mae


# In[10]:


def plot_cont(col, ax, color=PALETTE_7[0]):
    sns.histplot(data=comb_df, x=col,
                hue="set",ax=ax, hue_order=labels,
                common_norm=False, **histplot_hyperparams)
    
    ax_2 = ax.twinx()
    ax_2 = plot_cont_dot(
        comb_df.query('set=="train"'),
        col, TARGET, ax_2,
        color=color
    )
    
    ax_2 = plot_cont_dot(
        comb_df, col,
        TARGET, ax_2,
        color=color
    )


# In[11]:


class F1(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass
    
    def fit(self, x, y = None):
        return self
    
    def transform(self, x, y = None):
        return x.drop([MINOFUPPERRANGE, AVERAGEOFUPPERRANGE, AVERAGEOFLOWERRANGE, MAXOFUPPERRANGE, MAXOFLOWERRANGE], axis = 1)


# In[12]:


def show_distance(df, name = ''):
    dist = df.corr()
    dist_lk = ward(dist)
    
    # Establecer la paleta de colores personalizada
    set_link_color_palette(PALETTE_3)
    
    plt.figure(figsize = (8, 5), dpi = 300)
    dendro = dendrogram(dist_lk, labels=df.columns, leaf_rotation=75)
    plt.title(f"Feature Distance in {name} Dataset", size = 15, weight = "bold")
    plt.show()


# In[13]:


def show_distance(df, name=""):
    hierarchy.set_link_color_palette(PALETTE_3[1:2]*2)
    fig, ax =  plt.subplots(1, 1, figsize=(14, 6), dpi=120)
    correlations = df.corr()
    converted_corr = 1 - abs(correlations)
    Z = linkage(squareform(converted_corr), 'complete')

    dn = dendrogram(Z, labels=df.columns,  ax=ax, above_threshold_color=PALETTE_3[2], orientation='right')
    hierarchy.set_link_color_palette(None)
    plt.grid(axis='x')
    plt.title('Hierarchical clustering, Dendrogram')
    plt.show()


# In[14]:


def plot_cont_dot(
    df, column, target, ax,
    show_yticks=False, color=PALETTE_7[0]
):

    bins = pd.cut(df[column], bins=n_bins)
    bins = pd.IntervalIndex(bins)
    bins = (bins.left + bins.right) / 2
    target = df[target]
    target = target.groupby(bins).mean()
    target.plot(
        ax=ax, linestyle="",
        marker=".", color=color,
        label=f"Mean {target.name}"
    )
    ax.grid(visible=False)
    
    if not show_yticks:
        ax.get_yaxis().set_ticks([])
        
    return ax


# In[15]:


## Boolean values only
def pie_plot(df: pd.DataFrame, hover_temp: str = "Status: ",
            feature=TARGET, palette=[LIGTH_BLUE,"#221e8f"], color=[BLUE ,DARK_BLUE],
            title="Target distribution"):
#     df[feature] = df[feature].replace({0: "Not cancelled ", 1: "Cancelled"})
    target = df[[feature]].value_counts(normalize=True).sort_index().round(decimals=3)*100
    fig = go.Figure()
    
    fig.add_trace(go.Pie(labels=target.index, values=target, hole=.4,
                        sort=False, showlegend=True, marker=dict(colors=color, line=dict(color=palette,width=2)),
                        hovertemplate = "%{label} " + hover_temp + ": %{value:.2f}%<extra></extra>"))
    
    fig.update_layout(template=font_family, title=title, 
                  legend=dict(traceorder='reversed',y=1.05,x=0),
                  uniformtext_minsize=15, uniformtext_mode='hide',height=600)
    fig.show()


# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 3px solid #008F77">Import data</p>

# In[16]:


train_df = pd.read_csv(TRAIN_DIR)
test_df = pd.read_csv(TEST_DIR)
original_df = pd.read_csv(ORIGINAL_PATH)
submission_df = pd.read_csv(SUBMISSION_DIR)


# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 3px solid #008F77">EDA and data modification</p>

# In[17]:


data_description(train_df)
data_description(test_df)
data_description(original_df)


# In[18]:


train_df.columns


# In[19]:


train_df.head()


# In[20]:


test_df.head()


# In[21]:


train_df.drop(ID, axis=1, inplace=True)
test_df.drop(ID, axis=1, inplace=True)


# In[22]:


labels = ["train", "test", "original"]


# In[23]:


comb_df = pd.concat([
    train_df.assign(set=labels[0]),
    test_df.assign(set=labels[1]),
    original_df.assign(set=labels[2]),
])
comb_df.reset_index(inplace=True)


# In[24]:


display(show_corr_heatmap(train_df, "Train dataframe heatmap"))
display(show_corr_heatmap(test_df, "Test dataframe heatmap"))
display(show_corr_heatmap(original_df, "Original dataframe heatmap"))


# In[25]:


columns = test_df.columns
n_cols = 5
n_rows = math.ceil(len(columns)/n_cols)
fig, ax = plt.subplots(n_rows, n_cols, figsize=(20, n_rows*4))
ax = ax.flatten()

for i, column in enumerate(columns):
    plot_axes = [ax[i]]
          
    sns.kdeplot(
        train_df[column], label="Train PS dataframe",
        ax=ax[i], color=PALETTE_7_C[0], fill = True
    )

    sns.kdeplot(
        test_df[column], label="Test PS datafraem",
        ax=ax[i], color=PALETTE_7_C[1], fill = True
    )
    
    sns.kdeplot(
        original_df[column], label="Original train dataframe",
        ax=ax[i], color=PALETTE_7_C[2], fill = True
    )
    
    # titles
    ax[i].set_title(f"Distribution of {column}");
    ax[i].set_xlabel(None)
    
    plot_axes = [ax[i]]
    handles = []
    for plot_ax in plot_axes:
        handles += plot_ax.get_legend_handles_labels()[0]
        plot_ax.legend().remove()
    
for i in range(i+1, len(ax)):
    ax[i].axis("off")
    
fig.suptitle(f"Datasets Distributions comparation\n\n", fontsize=20, ha="center", fontweight="bold")
fig.legend(handles, labels, loc="upper center", bbox_to_anchor=(0.5, 0.9), fontsize=20, ncol=4)
plt.tight_layout()


# In[26]:


show_distance(train_df, 'Train')


# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 3px solid #008F77">Model</p>

# In[27]:


test_df = pd.read_csv(TEST_DIR)
test_df['data_type'] = 0

train_df = pd.read_csv(TRAIN_DIR)
train_df['data_type'] = 0

if INCLUDE_ORIGINAL:
    original_df = pd.read_csv(ORIGINAL_PATH)
    original_df['data_type'] = 1
    train_df = pd.concat([train_df, original_df.drop(columns=['Row#'])]).reset_index(drop=True)


features = (['MinOfLowerTRange', RAININGDAYS, FRUITMASS, FRUITSET, SEEDS, 'bumbles', 'clonesize']
            +  ([] if INCLUDE_ORIGINAL else ['data_type']))

def fe(df):
    df[RAININGDAYS] = np.select(
        condlist=[
            df[RAININGDAYS] == 26, 
            df[RAININGDAYS] == 33],
        choicelist= [24, 34],
        default=df[RAININGDAYS])
    
fe(train_df)
fe(test_df)


# In[28]:


unique_targets = np.unique(train_df["yield"])
def mattop_post_process(preds):
     return np.array([min(unique_targets, key = lambda x: abs(x - pred)) for pred in preds])


# In[29]:


def fold_mae(y, preds, data_type):
    return mean_absolute_error(y[data_type==0], preds[data_type==0])

def get_fi(automl, estimator_name, X, y):
    fi = pd.DataFrame({
        'estimator_name': [],
        'Feature': [],
        'Importance': []})
    
    try:
        est = automl.model.named_estimators_['lgbm']
        imp = permutation_importance(est, X, y)
        fi = pd.DataFrame({
            'Importance': imp.importances_mean,
            'Feature': X.columns})
    except:
        try:
            est = automl.model
            imp = permutation_importance(est, X, y)
            fi = pd.DataFrame({
                'Importance': imp.importances_mean,
                'Feature': X.columns})
        except:
            pass
    
    if len(fi) > 0:
        fi = fi.assign(estimator_name=estimator_name)
    
    return fi


# In[30]:


class ML_Fitr:
    def __init__(self, name, time_budget, estimator_list=[], best_config=None, n_folds=10):
        self.name = name
        self.time_budget = time_budget
        self.estimator_list = estimator_list
        self.best_config = best_config
        self.n_folds = n_folds

    def fit_automl(self, random_state, X, y):

        automl_settings = {
            "verbose": 0,
            "metric": 'mae',
            "ensemble": False,
            "eval_method": 'cv',
            "log_file_name": "",
            "seed": random_state,
            "task": 'regression',
            "n_splits": self.n_folds,
            "time_budget": self.time_budget,
            "estimator_list": self.estimator_list
        }

        # time budget is decreased on each fit.
        self.time_budget //= 1.5
        if self.time_budget < MIN_TIME_BUDGET:
            self.time_budget = MIN_TIME_BUDGET

        automl = AutoML()
        automl.add_learner(learner_name='my_lgbm', learner_class=LGBM_CL)
        automl_settings["estimator_list"] = ['my_lgbm' ]  # change the estimator list
        
        automl.fit(X, y, starting_points=self.best_config, **automl_settings)
        self.best_config = automl.best_config_per_estimator

        return automl


# In[31]:


from flaml.automl.model import LGBMEstimator

class LGBM_CL(LGBMEstimator):
    """customise l1 loss"""

    def __init__(self, **config):
        super().__init__(objective="regression_l1", **config)


# In[32]:


def fit_nsd_auto_ML(model_fitters, random_state, train_data, test_data, features):
    # Perform k-fold cross-validation using KFold
    k_fold = KFold(n_splits=N_OUTER_FOLDS, random_state=random_state, shuffle=True)

    fast_fis = []  # List to store feature importances
    oof_metrics = []  # List to store out-of-fold metrics
    test_preds = np.zeros(len(test_data))  # Array to store test predictions
    oof_preds = np.zeros(len(train_data))  # Array to store out-of-fold predictions

    print('OOF Metric: ', end='')
    
    # Iterate over each fold in k_fold
    for train_index, test_index in k_fold.split(train_data, train_data[TARGET]):
        tr, vl = train_data.loc[train_index], train_data.loc[test_index]
        
        # For each model:
        oof_pred = np.zeros(len(vl))  # Array to store out-of-fold predictions for the current model
        test_pred = np.zeros(len(test_data))  # Array to store test predictions for the current model
        best_loss = 0  # Variable to store the best loss for the current model
        
        for model_fitter in model_fitters:
            # Fit the AutoML model using the model_fitter's fit_automl method
            automl = model_fitter.fit_automl(random_state, tr.filter(features), tr[TARGET].values)
            
            # Generate predictions using the AutoML model
            oof_pred += automl.predict(vl.filter(features)) / len(model_fitters)
            test_pred += automl.predict(test_data.filter(features)) / len(model_fitters)
            best_loss += automl.best_loss / len(model_fitters)
            
            # Get feature importances for the current model and fold
            fast_fi = get_fi(automl, model_fitter.name, vl.filter(features), vl[TARGET])
            fast_fis.append(fast_fi)
        
        # Calculate the out-of-fold metric for the current fold
        oof_metric = fold_mae(vl[TARGET].values, oof_pred, vl['data_type'].values)
        
        # Print the out-of-fold metric for the current fold
        print(f'{oof_metric:4.1f}', end=' ')
        
        # Update the out-of-fold and test predictions with the predictions from the current fold
        oof_preds[test_index] += oof_pred
        test_preds += test_pred / N_OUTER_FOLDS
        
        fast_fis.append(fast_fi)
        oof_metrics.append(oof_metric)

    # Return the out-of-fold predictions, test predictions, feature importances, and out-of-fold metrics
    return oof_preds, test_preds, fast_fis, oof_metrics


# In[33]:


def fit(model_fitters, train_data, test_data, features):
    test_preds = np.zeros(len(test_data))  # Array to store test predictions
    oof_preds = np.zeros(len(train_data))  # Array to store out-of-fold predictions
    oof_metrics = []  # List to store overall out-of-fold metrics
    oof_fold_metrics = []  # List to store out-of-fold metrics for each repeat
    fast_fis = []  # List to store feature importances
    
    for i in range(N_REPEATS): 
        # Fit the models and generate predictions using fit_nsd_auto_ML
        oof_pred, test_pred, fast_fi, oof_fold_metric = fit_nsd_auto_ML(
            model_fitters,
            RANDOM_STATE + i,
            train_data.sample(frac=1),  # Shuffle the training data for each repeat
            test_data,
            features
        )

        # Calculate the overall out-of-fold metric for the current repeat
        oof_metric = fold_mae(train_data[TARGET].values, oof_pred, train_data['data_type'].values)
        
        # Calculate the overall out-of-fold metric with post-processing for the current repeat
        oof_metric_pp = fold_mae(train_data[TARGET].values, mattop_post_process(oof_pred), train_data['data_type'].values)

        # Update the test predictions and out-of-fold predictions with the predictions from the current repeat
        test_preds += test_pred / N_REPEATS
        oof_preds += oof_pred / N_REPEATS

        # Extend the lists of out-of-fold metrics and feature importances with the values from the current repeat
        oof_fold_metrics.extend(oof_fold_metric)
        fast_fis.extend(fast_fi)

        # Print the out-of-fold metrics for the current repeat
        print(f'Repeat {i}: {oof_metric:4.1f} postprocessing {oof_metric_pp:4.1f}')
        
    # Return the out-of-fold predictions, test predictions, out-of-fold metrics, and feature importances
    return oof_preds, test_preds, oof_fold_metrics, fast_fis


# In[34]:


tr = train_df.sample(frac=1, random_state=RANDOM_STATE)

model_fitter = ML_Fitr(
    name='model',
    time_budget= FIRST_TIME_BUDGET, 
    n_folds=5) 

model_fitter.best_config = {
    'my_lgbm': {'n_estimators': 265, 'num_leaves': 93, 'min_child_samples': 20, 
                'learning_rate': 0.055, 'log_max_bin': 10, 
                'colsample_bytree': 0.88, 'reg_alpha': 0.00098, 
                'reg_lambda': 0.016}}

model_fitter.fit_automl(
    RANDOM_STATE, 
    tr.filter(features), 
    tr[TARGET].values)

model_fitter.time_budget = MIN_TIME_BUDGET
model_fitter.n_folds = N_FOLDS

print(model_fitter.best_config)

models = [model_fitter]
oof_preds, test_preds, oof_fold_metrics, fast_fis = fit(models, train_df, test_df, features)


# In[35]:


# Create a DataFrame 'df_metrics' with the 'oof' column containing 'oof_fold_metrics'
df_metrics = pd.DataFrame({'oof': oof_fold_metrics})

# Concatenate the 'fast_fis' list of DataFrames into 'df_importance' and sort it by 'Importance'
df_importance = pd.concat(fast_fis).sort_values(['Importance'])

# Check if 'df_importance' has any rows
if len(df_importance) > 0:
    # Group 'df_importance' by 'Feature' and calculate the mean and standard deviation of 'Importance'
    df_importance_mean = (
        df_importance
        .groupby(['Feature'], as_index=False)
        .agg(mean_imp=('Importance', 'mean'), std_imp=('Importance', 'std'))
        .sort_values(['mean_imp'])
    )

    # Create a list of features from 'df_importance_mean'
    feature_list = df_importance_mean['Feature'].tolist()

    # Create a categorical variable 'feature_cat' using 'pd.Categorical' with ordered categories
    feature_cat = pd.Categorical(df_importance_mean['Feature'], categories=feature_list)

    # Assign additional columns to 'df_importance_mean' using 'assign' method
    df_importance_mean = df_importance_mean.assign(
        feature_cat=feature_cat,
        mean_imp_min=lambda x: x['mean_imp'] - 2 * x['std_imp'],
        mean_imp_max=lambda x: x['mean_imp'] + 2 * x['std_imp']
    )

    # Merge 'df_importance' with 'df_importance_mean' by filtering columns
    df_importance = df_importance.merge(df_importance_mean.filter(['Feature', 'feature_cat']))

    # Display the plot using 'ggplot' with aesthetics and themes, and data from 'df_importance_mean'
    print(
        ggplot(df_importance, aes(y='feature_cat', x='Importance')) +
        theme_light() +
        theme(figure_size=(6, 4)) +
        geom_point(mapping=aes(x='mean_imp'), colour='SteelBlue', data=df_importance_mean) +
        labs(
            y='',
            x='',
            title=(f'Feature Importance of permutation\n'
                   f'oof metric={np.mean(oof_fold_metrics):4.1f}')
        )
    )

# Check if 'df_importance' has any rows
if len(df_importance) > 0:
    # Display the top 20 rows of 'df_importance_mean' filtered by columns
    print(df_importance_mean.filter(['Feature', 'mean_imp', 'std_imp']).head(20))


# In[36]:


class RegWppr_PLS(PLSRegression):
    def transform(self, X):
        return super().transform(X)
    def fit_transform(self, X, Y):
        return self.fit(X,Y).transform(X)


# In[37]:


import lightgbm as lgb
# Read the training data from the specified directory and set the index column as 'ID'
train = pd.read_csv(TRAIN_DIR, index_col=ID)

# Read the test data from the specified directory and set the index column as 'ID'
test = pd.read_csv(TEST_DIR, index_col=ID)

# Read the 'origin' data from the specified path and set the index column as 'Row#'
origin = pd.read_csv(ORIGINAL_PATH, index_col="Row#")

# Set the index name as 'ID' for the 'origin' DataFrame
origin.index.name = ID

# Replace values in the 'RAININGDAYS' column of the 'train' DataFrame
train.loc[train[RAININGDAYS] == 26, RAININGDAYS] = 24

# Replace values in the 'RAININGDAYS' column of the 'test' DataFrame
test.loc[test[RAININGDAYS] == 33, RAININGDAYS] = 34

# Replace values in the 'MAXOFUPPERRANGE' column of the 'train', 'test', and 'origin' DataFrames
for df in [train, test, origin]:
    df.loc[df[MAXOFUPPERRANGE].isin([71.9, 79, 89]), MAXOFUPPERRANGE] = 86

# Display the value counts of the 'RAININGDAYS' column for the 'train', 'test', and 'origin' DataFrames
for df in [train, test, origin]:
    display(df[RAININGDAYS].value_counts())

# Display the value counts of the 'MAXOFUPPERRANGE' column for the 'train', 'test', and 'origin' DataFrames
for df in [train, test, origin]:
    display(df[MAXOFUPPERRANGE].value_counts())


# In[38]:


train =  pd.read_csv(TRAIN_DIR, index_col = ID)
test =  pd.read_csv(TEST_DIR, index_col = ID)
origin = pd.read_csv(ORIGINAL_PATH, index_col = "Row#")
origin.index.name = ID


# In[39]:


# Update the 'RAININGDAYS' column in the 'train' dataframe
train.loc[train[RAININGDAYS] == 26, RAININGDAYS] = 24

# Update the 'RAININGDAYS' column in the 'test' dataframe
test.loc[test[RAININGDAYS] == 33, RAININGDAYS] = 34

# Update the 'MAXOFUPPERRANGE' column in 'train', 'test', and 'origin' dataframes
for df in [train, test, origin]:
    df.loc[df[MAXOFUPPERRANGE].isin([71.9, 79, 89]), MAXOFUPPERRANGE] = 86

# Display the value counts of 'RAININGDAYS' and 'MAXOFUPPERRANGE' columns in 'train', 'test', and 'origin' dataframes
for df in [train, test, origin]:
    display(df[RAININGDAYS].value_counts())
    display(df[MAXOFUPPERRANGE].value_counts())


# In[40]:


# Define a class named 'FeatEng' that inherits from 'BaseEstimator' and 'TransformerMixin'
class FeatEng(BaseEstimator, TransformerMixin):
    
    def __init__(self, verbose=False, pca_fts=[SEEDS, FRUITMASS, FRUITSET], pls_fts=[SEEDS, FRUITMASS, FRUITSET]):
        """
        Initialize the FeatEng object.

        Parameters:
        - verbose (bool): Whether to enable verbose mode.
        - pca_fts (list): List of feature names for PCA transformation.
        - pls_fts (list): List of feature names for PLS transformation.
        """
        self.verbose = verbose
        
        # Number of PCA components and the feature names for PCA transformation
        self.pca_components = len(pca_fts)
        self.pca_fts = pca_fts
        
        # Number of PLS components and the feature names for PLS transformation
        self.pls_components = len(pls_fts)
        self.pls_fts = pls_fts
        

    def fit(self, x, y=None):
        """
        Fit the FeatEng object to the input data.

        Parameters:
        - x: Input data.
        - y: Target variable (optional).

        Returns:
        - self: The fitted FeatEng object.
        """
        # Create a pipeline for PCA transformation and fit it to the input data
        self.pipe_pca = make_pipeline(StandardScaler(), PCA(n_components=self.pca_components))
        self.pipe_pca.fit(x[self.pca_fts])
        
        # Create a pipeline for PLS transformation and fit it to the input data
        self.pipe_pls = make_pipeline(StandardScaler(), RegWppr_PLS(n_components=self.pls_components))
        self.pipe_pls.fit(x[self.pls_fts], x[TARGET])
        
        return self
    
    def transform(self, x, y=None):
        """
        Transform the input data using the fitted FeatEng object.

        Parameters:
        - x: Input data.
        - y: Target variable (optional).

        Returns:
        - df: Transformed data.
        """
        # Make a copy of the input data
        df = x.copy()
        
        # Generate column names for the PCA transformed features
        pca_cols = [f"pca_{i}" for i in range(self.pca_components)]
        
        # Perform PCA transformation on the selected features and assign the transformed values to new columns
        df[pca_cols] = self.pipe_pca.transform(df[self.pca_fts])
        
        # Generate column names for the PLS transformed features
        pls_cols = [f"pls_{i}" for i in range(self.pls_components)]
        
        # Perform PLS transformation on the selected features and assign the transformed values to new columns
        df[pls_cols] = self.pipe_pls.transform(df[self.pls_fts])
        
        return df


# In[41]:


def fit_lgbm_model(params={}, features=None, n_splits=5, seed=SEED, verbose=False,
                   use_original_dataframe=False, pca_fts=[SEEDS, FRUITMASS, FRUITSET],
                   pls_fts=[SEEDS, FRUITMASS, FRUITSET]):
    # If features is not provided, set it to the columns of the test dataframe
    if features is None:
        features = test.columns
    
    # Initialize lists to store best iteration, validation scores, and training scores
    best_iteration, validation_sc, train_sc = [], [], []
    
    # Dictionary to store evaluation results
    eval_result = {}
    
    # Define callbacks for LightGBM training
    callbacks = [lgb.early_stopping(100), lgb.record_evaluation(eval_result)]
    
    # Add log evaluation callback if verbose is True
    if verbose:
        callbacks.append(lgb.log_evaluation(200))
        
    # Initialize out-of-fold (oof) and prediction series
    oof = pd.Series(0, index=train.index)
    y_pred = pd.Series(0, index=test.index)
   
    # Perform k-fold cross-validation
    folds = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
    for fold, (trn_idx, val_idx) in enumerate(folds.split(train, train[TARGET])):
        # Initialize feature engineering object
        fe = FeatEng(pca_fts=pca_fts, pls_fts=pls_fts)
        
        # Perform feature engineering and create training and validation datasets
        if use_original_dataframe: 
            X_train = fe.fit_transform(pd.concat([train.loc[trn_idx], origin], axis=0))[features]
            y_train = pd.concat([train.loc[trn_idx], origin], axis=0)[TARGET]
        else:            
            X_train = fe.fit_transform(train.loc[trn_idx])[features]
            y_train = train.loc[trn_idx, TARGET]

        X_val, y_val = fe.transform(train.loc[val_idx])[features], train.loc[val_idx, TARGET]
        
        # Transform test dataset for prediction
        X_pred = fe.transform(test)[features]
        
        # Create LightGBM datasets
        dtrn = lgb.Dataset(X_train, label=y_train) 
        dval = lgb.Dataset(X_val, label=y_val)
   
        # Train the LightGBM model
        model = lgb.train(params, dtrn, num_boost_round=2000, valid_sets=dval, callbacks=callbacks)
        best_iteration.append(model.best_iteration)
    
        # Make predictions on validation and test sets
        oof.loc[val_idx] = model.predict(X_val, num_iteration=best_iteration[fold])
        y_pred += model.predict(X_pred, num_iteration=best_iteration[fold]) / n_splits
        
        # Calculate mean absolute error (MAE) for validation and training sets
        validation_sc.append(mean_absolute_error(y_val, oof.loc[val_idx]))
        train_sc.append(mean_absolute_error(y_train, model.predict(X_train)))
        
        # Print fold-wise results if verbose is True
        if verbose:
            print(f'Fold {fold + 1} // Valid: {validation_sc[fold]:.5f} // Train: {train_sc[fold]:.5f} // Best it.: {best_iteration[fold]:4}')
        
    # Print overall results
    print(f"OOF MAE: {mean_absolute_error(train[TARGET], oof):.5f} // Mean MAE: {np.mean(validation_sc):.5}")

    return {"oof":oof, "oof_score":mean_absolute_error(train[TARGET], oof), "overfeat":np.mean(train_sc) - np.mean(validation_sc),
           "best_iteration":best_iteration, "y_pred":y_pred}


# In[42]:


params = {
    'verbose':-1,
    'subsample': .7, 
    'max_bin': 1000, 
    'bagging_freq': 1,
    'random_state': SEED,
    'learning_rate': 0.05, 
    'colsample_bytree': .8, 
    'objective':'regression_l1',

}

result_1 = fit_lgbm_model(params, n_splits = 10, seed = SEED, verbose = True, use_original_dataframe = True, 
               pca_fts = [FRUITSET, SEEDS, FRUITMASS], 
              features=["pca_0", "pca_1", RAININGDAYS, FRUITMASS, MAXOFUPPERRANGE, FRUITSET, SEEDS])


print(f"MAE : {result_1['oof_score']:.5f}")


# In[43]:


result_2 = fit_lgbm_model(params, n_splits = 10, seed = SEED, verbose = True, use_original_dataframe = True, 
               pca_fts = [FRUITSET, SEEDS, FRUITMASS], pls_fts = [FRUITSET, SEEDS], 
              features=["pls_0", "pca_0", "pca_1", RAININGDAYS, FRUITMASS, MAXOFUPPERRANGE, FRUITSET, SEEDS])


# In[44]:


oofs = pd.DataFrame(index=train.index)  # Create an empty DataFrame to store the out-of-fold predictions
oofs = pd.concat([oofs, 
                  pd.Series(oof_preds[:train.shape[0]], index=train.index), 
                  result_1["oof"], 
                  result_2["oof"]], axis=1)  # Concatenate the out-of-fold predictions from different models
oofs.columns=["Patrick", "Alex1", "Alex2"]  # Assign column names to the DataFrame

preds = pd.DataFrame(index=test.index)  # Create an empty DataFrame to store the test predictions
preds = pd.concat([preds, 
                  pd.Series(test_preds, index=test.index), 
                  result_1["y_pred"], 
                  result_2["y_pred"]], axis=1)  # Concatenate the test predictions from different models
preds.columns=["Patrick", "Alex1", "Alex2"]  # Assign column names to the DataFrame

display(oofs)  # Display the DataFrame of out-of-fold predictions
display(preds)  # Display the DataFrame of test predictions


# In[45]:


ridge_bld = Ridge(positive = True)
ridge_bld.fit(oofs, train[TARGET])
print(f"MAE ridge Regression 3 OOFS predictions : {mean_absolute_error(train[TARGET], ridge_bld.predict(oofs))}\nCoefficients :")
display(pd.Series(ridge_bld.coef_.round(2), oofs.columns, name='weight'))


# In[46]:


LADReg_bld = LADRegression(positive = True)
LADReg_bld.fit(oofs, train[TARGET])
print(f"MAE LAD Regression 3 OOFS predictions : {mean_absolute_error(train[TARGET], LADReg_bld.predict(oofs))}\nCoefficients :")
display(pd.Series(LADReg_bld.coef_.round(2), oofs.columns, name='weight'))


# In[47]:


result_df = pd.Series([
    mean_absolute_error(train[TARGET], oofs["Patrick"]),
    mean_absolute_error(train[TARGET], oofs["Alex1"]),
    mean_absolute_error(train[TARGET], oofs["Alex2"]),
    mean_absolute_error(train[TARGET], oofs.mean(axis=1)),
    mean_absolute_error(train[TARGET], ridge_bld.predict(oofs)),
    mean_absolute_error(train[TARGET], LADReg_bld.predict(oofs)),
    ], index = ["Patrick", "Alex1", "Alex2", "Blend_Mean", "Blend_Ridge", "Blend_LAD"], name="MAE")
result_df


# 
# ## <p style="font-family:Consolas Mono; font-weight:normal; letter-spacing: 2px; color:#06D1C7; font-size:130%; text-align:left;padding: 0px; border-bottom: 3px solid #008F77">Send prediction</p>

# In[48]:


sub = pd.DataFrame(index=test.index)
sub[TARGET] = LADReg_bld.predict(preds)
sub[TARGET].to_csv("submission.csv")


# In[49]:


sub = pd.read_csv(SUBMISSION_DIR)
plt.figure(figsize=(5, 4))
plt.title('Predictions')
plt.hist(sub[TARGET], bins=100, color=PALETTE_7[0])
plt.show()

sub.head(10)

